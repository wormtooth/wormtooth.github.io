<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-G153DWZ7W5"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-G153DWZ7W5');
    </script>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport" />
    <meta content="yes" name="apple-mobile-web-app-capable" />
    <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style" />
    <meta content="telephone=no" name="format-detection" />
    <title>
    Âè∂Êüê‰∫∫ÁöÑÁ¢éÁ¢éÂøµ | Believe in Mathematics    </title>
    <link rel="stylesheet" type="text/css" href="/theme/css/style.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/pygment.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/normalize/6.0.0/normalize.min.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/pure-min.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/grids-responsive-min.css" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7674955363445536"
     crossorigin="anonymous"></script>
</head>

<body>
<div class="body_container">
    <div id="header">
        <div class="site-name">
            <a id="logo" href="/."> Âè∂Êüê‰∫∫ÁöÑÁ¢éÁ¢éÂøµ </a>
            <p class="description"> Believe in Mathematics </p>
        </div>
        <div id="nav-menu">
            <a href="/."><i class="fa fa-home"> Home</i></a>
            <a href="/movies/"><i class="fa fa-film"> Movies</i></a>
            <a href="/archives.html"><i class="fa fa-archive"> Archive</i></a>
        </div>
    </div>
    <div id="layout" class="pure-g">
        <div class="pure-u-1 pure-u-lg-3-4"><div class="content_container">
<div class="post">
    <h1 class="post-title"><a href="/20250517-aggregate-means-and-stds/">Aggregate Means and Standard Deviations</a></h1>
    <div class="post-meta">2025-05-17</div>
    <a data-disqus-identifier="20250517-aggregate-means-and-stds/" href="/20250517-aggregate-means-and-stds/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>Imagine that we need to monitor the average response time of a webpage and the standard deviation daily. This is a simple ask. At the end of the day, we will get a list of response times <span class="math">\(x_1, x_2, ..., x_n\)</span>. Then we use the formulas to get the mean <span class="math">\(\mu\)</span> and standard deviation <span class="math">\(\sigma\)</span>:<br>
</p>
<div class="math">$$\mu = \frac{\sum_i^n x_i}{n}, \text{ and } \sigma = \sqrt{\frac{\sum_i^n(x_i - \mu)^2}{n}}.$$</div>
<p>What if at the end of the month we want to get the mean and standard deviation for the whole month? Well, it is also a simple ask. The simpleset way is to pull the record of all response times and use the formulas again to compute the mean and standard deviation. But we already has computed the daily means and standard deviations. Can we aggregate the daily means and standard deviations into monthly statistics? More generally, can we aggregate means and standard deviations of subsets into that of the whole set? The answer is affirmative with the following formulas.</p>

    </div>
    <p class="readmore"><a href="/20250517-aggregate-means-and-stds/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20250413-lora-for-sequence-classification/">LoRA for Sequence Classification</a></h1>
    <div class="post-meta">2025-04-13</div>
    <a data-disqus-identifier="20250413-lora-for-sequence-classification/" href="/20250413-lora-for-sequence-classification/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>Low-rank adaption, or <a href="https://arxiv.org/abs/2106.09685">LoRA</a>, is another parameter-efficient fine-tuning techniques to align large models to specific tasks. The core idea is to approximate updates of a large matrix by two matrices of smaller rank:</p>
<p><img alt="Illustration of LoRA, from the original paper" src="https://wormtooth.com/images/20250413-lora.png"></p>
<p>The idea works becasue in the fine-tuning stage, the data we use is very small and narrowly-focused (on some speficic domain) compared to the pretrained data, and thus we can represent the updates with smaller matrices.</p>

    </div>
    <p class="readmore"><a href="/20250413-lora-for-sequence-classification/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20250323-prompt-tuning-for-sequence-classification/">Prompt Tuning for Sequence Classification</a></h1>
    <div class="post-meta">2025-03-23</div>
    <a data-disqus-identifier="20250323-prompt-tuning-for-sequence-classification/" href="/20250323-prompt-tuning-for-sequence-classification/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>In my previous blog post <a href="https://wormtooth.com/20250223-zero-shot-classification-with-llm/">Zero-Shot Text Classification with pretrained LLM</a>, I used Qwen2.5-0.5B-Instruct for sentiment analysis without any training. With some tweet on the prompts, we can see an improvement of accuracy from 77.5% to 82.5%. We might be able to squeeze the performance even more with prompt engineering, but it is inefficient as most of the time we don't know why one word is better than another in the prompts. Instead of prompt engineering, we can do <a href="https://arxiv.org/abs/2104.08691">prompt tuning</a> with some labelled data, which is one of the parameter-efficent ways to fine tune a LLM model. Its main idea is to prepend some tunable tokens to some task specific prompt while freezing the LLM model. We then train the embeddings of the prepended tokens on the labelled data so that the learned tokens can align the task specific prompt better to the task. </p>

    </div>
    <p class="readmore"><a href="/20250323-prompt-tuning-for-sequence-classification/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20250308-text-sequence-classification-with-apple-mlx/">Sequence Classification with Apple MLX</a></h1>
    <div class="post-meta">2025-03-08</div>
    <a data-disqus-identifier="20250308-text-sequence-classification-with-apple-mlx/" href="/20250308-text-sequence-classification-with-apple-mlx/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p><a href="https://github.com/ml-explore/mlx">MLX</a> is an array framework for machine learning on Apple silicon. The biggest advantage of the framework is the compatibility with the unified memory on Apple so that operations on MLX arrays can be performed on any of the supported device types without transferring data. It makes MLX a strong candidate when it comes to inferencing and even training a large model on Apple silicon. There are examples specifically designed for <a href="https://github.com/ml-explore/mlx-examples/tree/main/llms">LLM</a>, with a focus on text completion. As of today, there are few examples on other LLM tasks such as sequence classification for MLX since the framework is relatively new. I will provide an example to do classification inference with MLX, replicating what I did in my previous article <a href="https://wormtooth.com/20250223-zero-shot-classification-with-llm/">Zero-Shot Text Classification with pretrained LLM</a>.</p>

    </div>
    <p class="readmore"><a href="/20250308-text-sequence-classification-with-apple-mlx/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20250223-zero-shot-classification-with-llm/">Zero-Shot Text Classification with pretrained LLM</a></h1>
    <div class="post-meta">2025-02-23</div>
    <a data-disqus-identifier="20250223-zero-shot-classification-with-llm/" href="/20250223-zero-shot-classification-with-llm/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>According to this <a href="https://huggingface.co/tasks/zero-shot-classification">article</a>,</p>
<blockquote>
<p>Zero-shot text classification is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.</p>
</blockquote>
<p>Simply put, zero-shot text classification is to use preexisting models on classification tasks that the models are not trained upon. Large Language Models backed by <a href="https://arxiv.org/abs/1706.03762">attention</a> have a lot of great applications, such as summarization, chatbot, code completion and etc. It aslo gives zero-shot text classification a huge potential since most LLMs are pretrained on tremendous data which cover most common use case already. LLMs with strong reasoning capability such as deepseek can even perform well on unseen data. In this article, I want to discuss some pratical ways to use pretrained LLMs to do zero-shot classification using <a href="https://huggingface.co/docs/transformers/en/index">ü§ó Transformers</a>. </p>

    </div>
    <p class="readmore"><a href="/20250223-zero-shot-classification-with-llm/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20230904-pyspark-estimator-and-transformer/">PySpark Estimator and Transformer</a></h1>
    <div class="post-meta">2023-09-04</div>
    <a data-disqus-identifier="20230904-pyspark-estimator-and-transformer/" href="/20230904-pyspark-estimator-and-transformer/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>PySpark's <a href="https://spark.apache.org/docs/latest/ml-pipeline.html">pipeline</a> is a powerful tool that encapsulates machine learning processes. We can build rather complicated pipelines to our needs using the existing estimators/transformers come with the PySpark's library, until we can't. In this article, I will show how we can build custom estimators and transformers to make the pipeline even more powerful. </p>
<p>Imagine that we want to build a model with some high cardinality categorical features. Upon inspection, we find that only some most frequent values are useful and we decide to keep those frequent values and mask other values "OTHERS". We will implement <code>CardinalityReducer</code> that will keep only most frequent N values in a categorical column (or a column of string type). We will implement it in a way so that it can fit training sets together with other components in a pipeline.</p>

    </div>
    <p class="readmore"><a href="/20230904-pyspark-estimator-and-transformer/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20221229-save-bert-model-correctly/">Fixing an issue in saving/loading BERT models</a></h1>
    <div class="post-meta">2022-12-29</div>
    <a data-disqus-identifier="20221229-save-bert-model-correctly/" href="/20221229-save-bert-model-correctly/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>Recently I came across an issue in saving/loading <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> models with <a href="https://www.tensorflow.org">TensorFlow</a>. The BERT models are provided by the <a href="https://huggingface.co/docs/transformers/index">Transformers</a> library, and I used Tensorflow backend. When saving with <code>model.save(path)</code> then loading with <code>tf.keras.models.load_model(path)</code>, it gave the following TypeError or ValueError:</p>
<div class="highlight"><pre><span></span>TypeError/ValueError: The two structures don&#39;t have the same nested structure.
</pre></div>


<p>The article is to document several ways to solve the issue.</p>

    </div>
    <p class="readmore"><a href="/20221229-save-bert-model-correctly/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20210805-netflix-svd/">Simple SVD with Bias for Netflix Prize</a></h1>
    <div class="post-meta">2021-08-05</div>
    <a data-disqus-identifier="20210805-netflix-svd/" href="/20210805-netflix-svd/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>In my linear algebra class this summer, I used the <a href="https://www.netflixprize.com">Netflix Prize challenge</a> as a pratical example for an application of singular value decomposition (<a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a>). To be more precise, I explained the term <span class="math">\(p_u^Tq_i\)</span> in the simple SVD with bias model:<br>
</p>
<div class="math">$$\hat{r}_{ui} = \mu + b_u + b_i + p_u^Tq_i.$$</div>
<p><br>
The above model can be found in section 2.1 in this <a href="https://www.netflixprize.com/assets/ProgressPrize2008_BellKor.pdf">progress paper</a> of the winning team. In this note, I will explain this model and give an implementation in Python. A C implementation of the moddel can be found in my GitHub repository here: <a href="https://github.com/wormtooth/netflix_svd">https://github.com/wormtooth/netflix_svd</a>.</p>

    </div>
    <p class="readmore"><a href="/20210805-netflix-svd/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20201203-clustering-weibo-tags/">Clustering Weibo Tags</a></h1>
    <div class="post-meta">2020-12-03</div>
    <a data-disqus-identifier="20201203-clustering-weibo-tags/" href="/20201203-clustering-weibo-tags/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>I started a projected last October to collect Weibo's top search data (<a href="https://s.weibo.com/top/summary">ÂæÆÂçöÁÉ≠ÊêúÊ¶ú</a>) hourly. Together with the keywords or tags (ÂÖ≥ÈîÆËØç), most recent related weibos (or tweets) are collected as well. The result is save to a JSON file, with the format explained in this <a href="https://wormtooth.com/weibo-top-searches-json-format/">page</a>.</p>
<p>In this post, I would like to explore this data set and try to cluster tags. To be more precise, multiple tags can be used to refer to a same event, and these different tags are related and even share the same meaning. The task is to group similar tags together based on the data collected.</p>

    </div>
    <p class="readmore"><a href="/20201203-clustering-weibo-tags/">Read More</a></p>
</div>
<div class="post">
    <h1 class="post-title"><a href="/20201010-pca/">Principal Component Analysis</a></h1>
    <div class="post-meta">2020-10-10</div>
    <a data-disqus-identifier="20201010-pca/" href="/20201010-pca/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>This is the note I used as an example of applications in Linear Algebra I lectured at Purdue University. It is slightly modified so that it is more or less self contained.</p>
<p>Principal Component Analysis (PCA) is a linear algebra technique for data analysis, which is an application of eigenvalues and eigenvectors. PCA can be used in</p>
<ol>
<li>exploratory data analysis (visualizing the data)</li>
<li>features reduction</li>
</ol>
<p>We will learn the basic idea of PCA and see its applications in handwritten-digits recognition, eigenfaces and etc.</p>

    </div>
    <p class="readmore"><a href="/20201010-pca/">Read More</a></p>
</div>

<div class="page-navigator">
    <span class="page-number current">1</span>
    <a class="page-number" href="/page/2/">2</a>
    <a class="page-number" href="/page/3/">3</a>
    <span class="space">...</span>
    <a class="page-number" href="/page/11/">11</a>
    <a class="extend next" rel="next" href="/page/2/">Next</a></nav>
</div class="page-navigator">

    <script id="dsq-count-scr" src="//wormtooth.disqus.com/count.js" async></script>

        </div></div>
        <div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar">
<div class="widget">
    <form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form">
        <input type="text" name="q" maxlength="20" placeholder="Search"/>
        <input type="hidden" name="sitesearch" value="https://wormtooth.com"/>
    </form>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-heart-o"> Âπ¥ËΩªÁöÑÂøÉÂè™Êúâ‰∏ÄÈù¢</i></div>
    <img src="/images/mobius_heart.png" class="nofancybox" />
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-paper-plane-o"> ÂøÉÊÉÖÈöèÁ¨î</i></div>
    <p>Êú™Êù•‰ºöÊù•ÔΩû</p>
    <span class="qed"><a href="/scribble">View All</a></span>
</div><div class="widget">
    <div class="widget-title">
        <i class="fa fa-folder-o"> Categories</i>
    </div>
    <ul class="category-list">
        <li class="category-list-item"><a class="category-list-link" href="/category/life/">Life</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/machine-learning/">Machine Learning</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/mathematics/">Mathematics</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/notes/">Notes</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/programming/">Programming</a></li>
    </ul>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div>
    <ul></ul>
    <a href="https://yongjiasong.com/" title="JOY DOMAIN" target="_blank">JOY DOMAIN</a>
</div>
<div class="widget">
    <div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div>
    <script type="text/javascript" src="//wormtooth.disqus.com/recent_comments_widget.js?num_items=5&hide_avatars=1&avatar_size=32&excerpt_length=20&hide_mods=1"></script>
</div>
        </div></div>
        <div class="pure-u-1 pure-u-lg-3-4">
<div id="footer">Copyright ¬© 2025 <a href="/." rel="nofollow">Âè∂Êüê‰∫∫ÁöÑÁ¢éÁ¢éÂøµ.</a> <a rel="nofollow" target="_blank" href="https://getpelican.com/">Pelican</a> &amp; <a rel="nofollow", target="_blank", href="https://github.com/wormtooth/maupassant-pelican">maupassant</a>.</div>        </div>
    </div>
</div>
<a id="rocket" href="#top" class="show"></a>
<script type="text/javascript" src="/theme/js/totop.js" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.js" async></script>
<script type="text/javascript" src="/theme/js/fancybox.js" async></script>
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.css" />

<script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      },
      Macros: {
        N: "\\mathbb{N}",
        Z: "\\mathbb{Z}",
        Q: "\\mathbb{Q}",
        R: "\\mathbb{R}",
        C: "\\mathbb{C}"
      }
    },
    'HTML-CSS': {
      imageFont: null
    }
  });
</script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.1.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>