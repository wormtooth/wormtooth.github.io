<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport" />
    <meta content="yes" name="apple-mobile-web-app-capable" />
    <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style" />
    <meta content="telephone=no" name="format-detection" />
    <title>
[npnet] 线性回归 | 叶某人的碎碎念    </title>
    <link rel="stylesheet" type="text/css" href="/theme/css/style.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/pygment.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css" />
    <link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css" />
    <link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css" />
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
    <link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico" />
</head>

<body>
<div class="body_container">
    <div id="header">
        <div class="site-name">
            <a id="logo" href="/."> 叶某人的碎碎念 </a>
            <p class="description"> Believe in Mathematics </p>
        </div>
        <div id="nav-menu">
            <a href="/."><i class="fa fa-home"> Home</i></a>
            <a href="/movies/"><i class="fa fa-film"> Movies</i></a>
            <a href="/archives.html"><i class="fa fa-archive"> Archive</i></a>
        </div>
    </div>
    <div id="layout" class="pure-g">
        <div class="pure-u-1 pure-u-lg-3-4"><div class="content_container">
<div class="post">
    <h1 class="post-title">[npnet] 线性回归</h1>
    <div class="post-meta">Oct 05, 2019
    <span> | </span> <span>Machine Learning</span>
    </div>
    <a data-disqus-identifier="npnet1/" href="/npnet1/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>我们现在已经知道机器学习三个重要的模块是<strong>模型 (model)</strong>、<strong>标准 (criterion)</strong> 以及 <strong>优化 (optimization)</strong>。在这篇笔记中我们讨论各个模块必要的基本功能，然后用 Python 给出模块的基类 (base class)。 最后我们继承这些基类实现上一篇笔记的线性模型 (linear model) 以及均方误差 (mean square error)。</p>
<!--more-->

<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="n">run</span> <span class="n">import_npnet</span><span class="o">.</span><span class="n">py</span>
<span class="n">npnet</span> <span class="o">=</span> <span class="n">import_npnet</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<h1>模型</h1>
<p>一个模型抽象来看是一个函数 <span class="math">\(f(x; a)\)</span>，其中 <span class="math">\(a\)</span> 是模型的参数。因此一个模型必须要保存参数其自身的参数，以及实现函数 <span class="math">\(f(x; a)\)</span> 的功能 （也叫正向传播/前馈 (<a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward</a>)）。另外，我们还需要损失函数对参数的偏导数来进行梯度下降，所以我们还需要在模型中实现反向传播 (<a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>) 并保存计算好的偏导数。</p>
<div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradients</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                <span class="n">grads</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="s2">&quot;&lt;{self.__class__.__name__}&gt;&quot;</span>
</pre></div>


<p><code>Model</code> 中定义了两个属性 (property) <code>Model._parameters</code> 和 <code>Model._gradients</code>。这两个属性返回对应属性的列表 (list) 形式。比如 <code>Model._parameters</code> 检查 <code>Model.parameters</code> 是否为列表，若是则直接返回，若不是则返回只有其一个元素的列表。这两个属性是为了方便以后的实现。我们还定义了 <code>Model._init_gradients</code>。当我们定义好 <code>Model.parameters</code> 之后，我们可以用这个方法初始化所有偏导数为0。</p>
<h2>线性回归</h2>
<p>线性回归模型的数学表达式是<br>
</p>
<div class="math">$$y = x W + b,$$</div>
<p><br>
其中 <span class="math">\(x\)</span> 是长度为 <span class="math">\(m\)</span> 的行向量 (row vector)，<span class="math">\(y\)</span> 是长度为 <span class="math">\(n\)</span> 的行向量，<span class="math">\(W\)</span> 是 <span class="math">\(m \times n\)</span> 的矩阵 (matrix)，<span class="math">\(b\)</span> 是长度为 <span class="math">\(n\)</span> 的行向量，最后 <span class="math">\(x W\)</span> 是矩阵乘法，<span class="math">\(+\)</span> 是向量加法。在这个模型中，<span class="math">\(W\)</span> 和 <span class="math">\(b\)</span> 是模型参数。在 <code>numpy</code> 中，我们使用 <code>@</code> 代表矩阵乘法。知道这点，我们很容易写出 <code>Linear.forward</code> 的代码。难点在于如何正确地用 <code>numpy</code> 计算参数 <code>W</code> 和 <code>b</code> 对于损失函数的偏导数。</p>
<p>在实际应用中，<span class="math">\(x\)</span> 并只是一个行向量，而是由若干个行向量形成的矩阵。假设 <span class="math">\(x\)</span> 是一个 <span class="math">\(l \times m\)</span> 矩阵。这说的是 <span class="math">\(x\)</span> 是由 <span class="math">\(l\)</span> 个长度为 <span class="math">\(m\)</span> 的行向量构成的。因为 <code>numpy</code> 的广播 (broadcasting) 机制，我们可以将这样一个 <span class="math">\(l \times m\)</span> 矩阵 <span class="math">\(x\)</span> 作为输入，得到一个 <span class="math">\(l \times n\)</span> 的矩阵 <span class="math">\(y\)</span>。<span class="math">\(y\)</span> 每一行是 <span class="math">\(x\)</span> 相对应的那行代入上述线性公式得到的。这样我们相当于可以同时计算多个输入。这是便利，也为我们的反向传播的实现增加了难度。</p>
<p>假设观测值也组成一个 <span class="math">\(l \times n\)</span> 矩阵 <span class="math">\(\tilde{y}\)</span>。我们记 <span class="math">\(x = (x_{ij}), y = (y_{ij}), W = (W_{ij}), b = (b_1, \dots, b_n)\)</span>. 展开 <span class="math">\(y = xW + b\)</span> 我们得到<br>
</p>
<div class="math">$$y_{ij} = \sum_{k=1}^m x_{ik} W_{kj} + b_j.$$</div>
<p><br>
根据反向传播的过程，我们会有一个 <span class="math">\(l \times n\)</span> 矩阵 <span class="math">\(g = (g_{ij})\)</span> 作为后面传递回来的梯度。这个传递回来的梯度的矩阵大小是跟我们模型的矩阵大小是一致的。假设最终的损失是 <span class="math">\(L\)</span>。那么对于每个 <span class="math">\(b_h\)</span>，我们有<br>
</p>
<div class="math">$$\frac{\partial L}{\partial b_h} = \sum_{i, j} \frac{\partial L}{\partial y_{ij}} \cdot \frac{\partial y_{ij}}{\partial b_h} = \sum_{ij} g_{i,j}\delta_{jh} = \sum_{i=1}^l g_{ih}.$$</div>
<p> <br>
这里 <span class="math">\(\delta_{ij} = 1\)</span> 当且仅当 <span class="math">\(i = j\)</span>，否则等于0。同样地，我们对每个 <span class="math">\(W_{uv}\)</span>,<br>
</p>
<div class="math">$$\frac{\partial L}{\partial W_{uv}} = \sum_{i, j} \frac{\partial L}{\partial y_{ij}} \cdot \frac{\partial y_{ij}}{W_{uv}} = \sum_{i, j} g_{ij} x_{iu} \delta_{jv} = \sum_{i=1}^l g_{iv}x_{iu}.$$</div>
<p>通过上面的计算，我们发现对于 <span class="math">\(b\)</span> 的偏导数我们可以将 <span class="math">\(g\)</span> 的每一行加起来得到，而对于 <span class="math">\(W\)</span> 的偏导数，我们可以简单地用矩阵乘法 <span class="math">\(\frac{\partial L}{\partial W} = x^T g\)</span> 来表示。其中 <span class="math">\(x^T\)</span> 是 <span class="math">\(x\)</span> 的转置矩阵。至此我们已经成功推出参数的偏导数公式。但是我们还需要求出对于输入 <span class="math">\(x\)</span> 的偏导数，作为梯度传递到前面去以继续反向传播的过程。对每个 <span class="math">\(x_{uv}\)</span>，<br>
</p>
<div class="math">$$\frac{\partial L}{\partial x_{uv}} = \sum_{i, j} \frac{\partial L}{\partial y_{ij}} \cdot \frac{\partial y_{ij}}{\partial x_{uv}} = \sum_{i, j} g_{ij}W_{vj}\delta_{iu} = \sum_{j=1}^n g_{uj}W_{vj}.$$</div>
<p><br>
也就是说需要反向传播回去的梯度是 <span class="math">\(\frac{\partial L}{\partial x} = g W^T\)</span>。</p>
<p>根据上面的推导公式，我们可以得到一下线性模型的实现。值得注意的是参数 <span class="math">\(W\)</span> 的初始化，我使用的是 He initialization 。选择这样的初始化是因为后面我们常用的是 <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> 激活函数。在使用 ReLU 时，这样的初始化有助于模型更快地收敛。</p>
<div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span>
                         <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">)</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span>

        <span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">]</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">))</span>
        <span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_gradients</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span> <span class="err">@</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
</pre></div>


<h1>标准</h1>
<div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="k">class</span> <span class="nc">Criterion</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="s2">&quot;&lt;{self.__class__.__name__}&gt;&quot;</span>
</pre></div>


<h2>均方误差</h2>
<p>假设 <span class="math">\(y\)</span> 是预测值，<span class="math">\(\tilde{y}\)</span> 是观测值。均方误差的数学公式是<br>
</p>
<div class="math">$$L(y, \tilde{y}) = \frac{1}{2} |y - \tilde{y}|_2^2.$$</div>
<p><br>
这里的 <span class="math">\(|x|_2^2\)</span> 是求 <span class="math">\(x\)</span> 的 L2 范式 (L2 norm) 的平方，是平方函数的向量推广。如果 <span class="math">\(x = (x_1, \dots, x_n)\)</span>， 那么<br>
</p>
<div class="math">$$|x|_2^2 = \sum_{i=1}^n x_i^2.$$</div>
<p>我们需要计算 <span class="math">\(\frac{\partial L}{\partial y}\)</span>：<br>
</p>
<div class="math">$$\frac{\partial L}{\partial y} = y - \tilde{y}.$$</div>
<div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="k">class</span> <span class="nc">MSELoss</span><span class="p">(</span><span class="n">Criterion</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">output</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">/=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">/</span> <span class="n">n</span>
</pre></div>


<p>我们发现 <code>MSELoss.forward</code> 和 <code>MSELoss.backward</code> 都除以了 <code>n = output.shape[0]</code>。这是因为 <code>targt</code> 是将所有观测到的 <code>y</code> 放在一起形成的矩阵。<code>target</code> 的每一行代表一个观测点的 <code>y</code> 值。同理，<code>output</code> 的每一行代表对一个观测点的预测。按照之前我们的讨论，总的误差是所有观测点误差的平均值，所以要除以观测点的个数。</p>
<h1>优化器</h1>
<p>我们在后面的笔记中实现类似 <a href="https://pytorch.org/docs/stable/nn.html#sequential"><code>torch.nn.Sequential</code></a> 的简单神经网络。其本质就是将一些简单的模型串起来形成一个更大更深的模型。考虑到这种串型网络，我们将一个基本模型看作包含一个基本模型的串型网络。优化器 (Optimizer) 要将所有模型的参数以及它们的偏导数放在各自的列表来获取统一处理的便利。</p>
<div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_gradients</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">grads</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">g</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="s2">&quot;&lt;{self.__class__.__name__}&gt;&quot;</span>
</pre></div>


<h2>随机梯度下降</h2>
<p>我们之前说的梯度下降是使用所有的观测点来计算各个参数的偏导数，才进行一次参数更新。而随机梯度下降 (<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a>) 是每次使用一个观测点去进行梯度下降法，也就是只使用一个观测点来计算各个参数的偏导数。随机梯度下降每次计算的梯度必然没有梯度下降计算的梯度准确，但是它使用了更小的内存，而且被证明能达到梯度下降的效果。</p>
<p>在我们的实现中，优化与梯度的计算是分开的，所以随机梯度下降与梯度下降在实现中并没有区别。我选择了 <code>SGD</code> 这个更常见的名字来命名这个实现。</p>
<div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
        <span class="k">for</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
</pre></div>


<h1>导数验证</h1>
<p>每当我们实现一个模型时，我们最担心的就是能否正确地计算偏导数。所以我们需要用数值方法计算导数来验证 <code>Model.backward</code> 的正确性。给定一个函数 <span class="math">\(f(x)\)</span>，我们可以通过以下公式估算 <span class="math">\(f^\prime(x)\)</span>：<br>
</p>
<div class="math">$$\frac{f(x+h) - f(x-h)}{2h}.$$</div>
<p><br>
这个想法能推广到多元函数。</p>
<div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="k">class</span> <span class="nc">GradientCheck</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">sample</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">allowed_error</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allowed_error</span> <span class="o">=</span> <span class="n">allowed_error</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">check_gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_gradient</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">check_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_parameters</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_gradients</span>
        <span class="k">for</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">check_gradient</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="n">name</span><span class="p">]):</span>
                    <span class="k">return</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">True</span>

    <span class="k">def</span> <span class="nf">check_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">comp</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">comp</span><span class="o">.</span><span class="n">shape</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span>
        <span class="n">criterion</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
        <span class="n">target</span><span class="p">,</span> <span class="n">allowed_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">allowed_error</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
            <span class="n">point</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">)</span>
            <span class="n">comp</span><span class="p">[</span><span class="n">point</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">high</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">comp</span><span class="p">[</span><span class="n">point</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">h</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">low</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">comp</span><span class="p">[</span><span class="n">point</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>
            <span class="n">numerical_grad_at_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
            <span class="n">analytic_grad_at_point</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="n">point</span><span class="p">]</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">numerical_grad_at_point</span> <span class="o">-</span> <span class="n">analytic_grad_at_point</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">error</span> <span class="o">&gt;</span> <span class="n">allowed_error</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">True</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 每次调用 self.model.forward 之前都会先调用此方法</span>
        <span class="k">pass</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">GradientCheck</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">check</span><span class="p">()</span>
</pre></div>


<pre>True</pre>

<h1>直线拟合</h1>
<p>在上一篇笔记里我们已经用两种方法做过了简单的直线拟合。我们现在使用 <code>Linear</code> 、<code>MSELoss</code> 和 <code>SGD</code> 来重新做一遍直线拟合。首先我们先生成数据。</p>
<div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img src="data:image/png;base64,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
"/></p>
<p>因为我们将机器学习的三个重要组成部分模块化了，所以使用起来也非常方便以及统一。我将这个过程用注释标记在下面的代码例子中。</p>
<div class="highlight"><pre><span></span><span class="c1">#建立模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># 记录下当前模型的初始值供后面的使用</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 选定超参数 (hyperparameter)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># 选择标准以及优化器</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># 训练模型</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 预测</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 计算损失</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">criterion</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 得到反向传播的初始梯度</span>
    <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># 清除之前的偏导数</span>
    <span class="n">model</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># 通过反向传播计算参数的偏导数</span>
    <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># 使用优化器更新参数</span>

<span class="c1"># 检测模型</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3RU1frG8e8mQAAbIFz1CiQq0kS8aizYrwWCDRGwgXoFL4gQIIiIxguWBFCpCioRUK/kooJgAVIAaQoqoNhooSQBxR8oEEsgELJ/f+woSQxFZiZnJvN81mKROXMm512z9Fmb9+y9j7HWIiIioaeS1wWIiMjRUYCLiIQoBbiISIhSgIuIhCgFuIhIiKpcnherU6eOjY6OLs9LioiEvBUrVvxora1b+ni5Bnh0dDTLly8vz0uKiIQ8Y0x2WcfVQhERCVEKcBGREKUAFxEJUQpwEZEQpQAXEQlRCnARkRClABcRCVEKcBGRALEWXnsNZswIzO9XgIuIBEBWFsTGwn33weTJgbmGAlxExI8KC2HsWGjeHJYscT9PnRqYa5XrUnoRkYps7Vro2hU+/hhat4bx4yEqKnDX0whcRMRH+/bBsGFwzjmwahW8/jqkpgY2vOEIAtwYM8kYs80Y802xY88ZY9YYY74yxswwxtQMbJkiIsFp5Uq46CJ49FG48UYX4PfcA8YE/tpHMgJ/DYgtdWwO0Nxa2wJYBzzq57pERILanj3w+ONwwQXw/fcwbZr7c/LJ5VfDYQPcWrsI2FHqWIa1tqDo5SdAvQDUJiISlJYsgXPPhaQk6NzZjbrbty/jxJQUiI6GSpXc3ykpfq3DHz3wLkCqH36PiEhQ+/VX6NMHLrsM8vIgLQ1efRVq1y7j5JQU6NYNsrPdhPDsbPfajyHuU4AbYxKAAuCgFRljuhljlhtjlm/fvt2Xy4mIeGbuXDj7bHj+eejZE775xs00OaiEBJfyxeXlueN+ctQBboz5F3Aj0Mlaaw92nrU22VobY62NqVv3T08EEhEJart2uamB110HVavC4sXwwgtw3HGH+WBODrsrw5NXwvjzSx73l6OaB26MiQUGAFdaa/MOd76ISCh691148EHYtg0GDoRBg6B69SP77MxL6tDn/O1srA3dij9JskEDv9V3JNMIpwBLgcbGmC3GmK7AWOA4YI4xZqUx5mW/VSQi4rFt2+D226FdO/jb3+DTT2Ho0CML7407N3LzlJu56brtRBYa5r0O42cWvVmjhrvz6SeHHYFba+8s4/BEv1UgIhIkrHX3GPv0cTcsExNhwACoUuXwn929bzfPfvwsQz8aSuVKlXn22mfps+lvVP1gMJgcN/JOSoJOnfxWr5bSi4gAmzfDAw/A7Nlw8cUwcSI0a3Zkn525biZ90vqwcedGbj/rdoa3Gk694+vBpUDnewNWswJcRMJaYSEkJ7uR9v79MHo09OoFERGH/+ymnZvok9aHD9Z9QNM6TZl3zzyuPu3qwBddRAEuImErMxP+/W9YuBCuuQZeeQVOO+3wn/u9XTLs42FEmAjXLrm4D1Ujqga+6GIU4CISdgoK3Ej7P/+ByEiYMAG6dDmy/UsO2i7xgAJcRMLK11+7ed3LlkHbtvDii/D3vx/+c163S8qiABeRsLB3LwwZ4v7UrAlvvgm33Xb4UXewtEvKogAXkQrvs89ci+Tbb90svtGjoU6dw39u1rpZ9E7rHRTtkrLogQ4iUmHl5UH//tCyJeTmwsyZ7vmUhwvvTTs3cfOUm7lxyo1ERkQy7555vNnhzaAKb9AIXEQqqAUL4P77YcMG6N4dnn0Wjj/+0J8J5nZJWRTgIlKh5Oa6Od3JyXDGGTB/Plx11eE/F+ztkrIowEWkwpg1y422t251rZMnn3TbjxxKMM4uOVIKcBEJeT/+CH37un1MmjeH6dPhwgsP/Zk9BXv+2LskFNolZVGAi0jIshbeegvi4lzrZPBgeOwxt2/3oYRiu6QsCnARCUnffw89esD777sHC0+c6J6Ycyih3C4piwJcREKKtTBpEjz0EOTnw/Dhrn1yqM2nKkK7pCwKcBEJGRs3uucCz5sHV17p9jBp2PDQn6ko7ZKyKMBFJOjt3++eQ5mQ4EbaL7/sdhGsdIiliJt2bqJvel/eX/t+hWiXlEUBLiJBbfVqt/nU0qVw/fUuvOvXP/j5FbVdUhYFuIgEpX373OrJp55yT4CfPBnuuuvQm09V5HZJWRTgIhJ0Pv/cbT715Zdux8AXXnAPFz6Y0u2SuXfP5ZrTrym/gj2iABeRoLF7t1s9OXy4C+wZM+CWWw5+fji1S8py2AA3xkwCbgS2WWubFx2rDbwFRANZwG3W2p2BK1NEKrqPPnK97nXr3N/PPQe1ah38/HBrl5TlSLaTfQ2ILXVsIDDPWnsmMK/otYjIX/bLL+4hwpdf7h66MGeOmx54sPDetHMTbd9s+8dWr3PvnhuUW72Wh8OOwK21i4wx0aUOtwWuKvr5dWAB8Igf6xKRMJCe7uZ1b94MffpAYiIce2zZ54Z7u6QsR9sDP8lau7Xo5x+Akw52ojGmG9ANoEGDBkd5ORGpSHbsgH794PXXoUkT1z655JKDnz9r3Sz6pPVhw84NYdsuKYvPT+Sx1lrAHuL9ZGttjLU2pm7dur5eTkSCXUoKREe7VTbR0e51Me+8A82auWmBCQnwxRcHD+/i7ZIqEVXCul1SlqMdgf+fMeYUa+1WY8wpwDZ/FiUiISolxfVE8vLc6+xs9xr44ZpO9OrlAvzccyEtDf7xj7J/jdolR+ZoA/x94F5gWNHf7/mtIhEJXQkJB8K7iM3L4799Pie+sBN5eTB0qNuIqkqVsn+F2iVH7kimEU7B3bCsY4zZAgzGBffbxpiuQDZwWyCLFJEQkZNT8iX16c540n5qw6WXutklTZqU/dHii3Ga1GkSNotxfHEks1DuPMhb+mZFpKQGDSA7m0IML9GDgQzDYnih1iAeXPRUmZtP7SnYw3MfP8eQj4aoXfIX+XwTU0TkD0lJrKvWgqtYQC/G0ZKlfFPtAnq90LjM8J6dOZvmLzZn0IJBtG3cljW91vDwpQ8rvI+QltKLiF8UFMCILZ0YvP8Oqlf6lVcL7+PeBvMxQ5KgU6cS52btyqJvWl/eW/ue2iU+UICLiM++/NJtPvX553DrrRGMG3cCJ5/86p/OU7vEv9RCEakoDjP/OhDy8+E//4GYGNiyBaZOddMETz75z+cWb5fc3PjmwLZLPPguvKARuEhFcIj516XbF/7yySdu1L16NdxzD4wcCSee+Ofzyr1d4sF34RXjFlKWj5iYGLt8+fJyu55I2IiOdkFVWlQUZGX59VK//QaPPw5jxkC9ejB+PLRp8+fzSrdLBl05iL4X9w18u6Qcv4vyYoxZYa2NKX1cI3CRiqDU/OvDHj9K8+a5Z1Fu2gQPPgjDhrmn5ZQ2O3M2vVN7s2HnBm476zZGtBpRfotxyum7CAbqgYtUBAfbKM5PG8jt2uWC+9proXJlWLgQxo37c3hn7criljdv4Yb/3fDH3iVvdXirfFdSBvi7CCYKcJGKICkJatQoeaxGDXfcR++/D2edBZMmwYABbsbJFVeUPGdPwR6eXvg0Tcc1Ze7GuTxz7TN8+cCX3kwNDOB3EXSsteX25/zzz7ciEiCTJ1sbFWWtMe7vyZN9+nXbtll7xx3WgrUtWli7bFnZ581aN8ueMeYMyxPY26beZjfnbvbpun7h5+/Ca8ByW0am6iamiJRgLUyZAr17w88/u2mCjzwCVUvdeyw9u+SFNi9w7enXelN0BaebmCJyWFu2QI8eMHMmXHQRTJzo2ifFlZ5d8sy1z5TP7BL5EwW4iGAtvPIKPPww7Nvn5nT37g0RESXP83R2ifyJAlwkzG3Y4GaYzJ8PV1/tgvz000ueU7pdMufuOWqXBAEFuEiY2r/fLcZ5/HH3cIXkZLj/fjDmwDlqlwQ3BbhIGPr2W7cM/rPP4Kab4KWX4NRTS56TmplKXGqc2iVBTPPARcLI3r3w1FPumZQbN7rZJu+9VzK8f1+Mc/3/rqdypcrMuXtO+S/GkSOiEbhImFi2DLp2ha+/hjvvdO2TunUPvK92SehRgItUcLt3w+DBMGKE2+b1/fdd26S44u2Sjs06MqLVCOqfUN+bguWIKcBFKrBFi9yoe/16N9PkuefghBMOvF98dknjExtrdkmI8SnAjTHxwP2ABb4G7rPW7vFHYSJy9H7+GQYOdDcnTz/d7SJ49dUH3t9TsIfhS4aTtDhJ7ZIQdtQBbow5FegNNLPW7jbGvA3cAbzmp9pE5CikpkL37m5VZXw8PP00HHNMsffVLqkwfG2hVAaqG2P2ATWA730vSUSOxk8/ucB+4w1o1gyWLIGLLz7wvtolFc9RB7i19jtjzHAgB9gNZFhrM0qfZ4zpBnQDaFAB9+MV8Zq1MG0a9OoFO3a4zacSEiAy0r1fvF1SyVRi2DXDiG8Zr3ZJBeBLC6UW0BY4DdgFTDXGdLbWTi5+nrU2GUgGtxuhD7WKSClbt7on47z7Lpx/PsyZAy1aHHhf7ZKKzZeFPNcCm6y12621+4DpwCX+KUtEDsVaePVV1ypJS4NnnnEPGf49vMtajPN2x7cV3hWMLz3wHOBiY0wNXAvlGkCbfYsEWFaWe8j6nDlw+eUwYQI0auTeU7skvPjSA//UGDMN+BwoAL6gqFUiIv5XWOieQ/noo27DqRdfdLNNKhX9Ozo1M5Xeab1Zv2O92iVhwqdZKNbawcBgP9UiIgexZo1bkLNkCcTGwvjxB57Rm7Uri/j0eN5d8y6NT2xMRucMrjvjOm8LlnKhlZgiQWzfPhg+HJ580j2X9/XX4e673Qhc7RJRgIsEqS++cKPuL76ADh1g7Fg46ST3nmaXCGg7WZGgs2cPPPYYXHCBmyb4zjswdaoL76xdWbR7q90fs0syOmdodkkY0whcJIgsWeJG3WvWwL/+5Z5NWauW2iVSNgW4SBD49Vc36h471t2cTE+HVq3ce8XbJR2adWBkq5EacQugABfx3Jw5bqvXnBy3HH7IEDj2WM0ukcNTgIt4ZOdOeOght6KycWO3d/dll7l2SeIitUvk8BTgIh6YMcPtYbJ9u9u3e/BgqFZN7RL5axTgIuXo//4P4uLcrJJ//ANmzYLzzitql7zn2iWNTmykdokcEQW4SDmwFiZPhr593Q3LpCR4+GHYb0q2S4ZeM5T4i+OJrBzpdckSAhTgIgGWkwMPPOCelNOyJUycCE2bql0ivtNCHpEAKSx0z6Q86yxYuBDGjIHFi6H6KQcW40RUiiCjcwZTO05VeMtfphG4SABkZsL997uZJddeC8nJcEr9PQz9WO0S8R8FuIgfFRTAqFEwaJB7pNnEiXDffZC2PpXrXjzQLhnRagQNTtAjBsU3CnARP/nqK7cMfvlyaNvW7de9t0YWt76t2SUSGOqBi/go/7UpDK45hvPP2Uf25z/yVq/FTJm6h0nrE2k6rikZGzIYes1QvnrgK4W3+JVG4CI++PTJNLo+eQ7f2mZ0YjKjC/uybP6vnP3cCWzYt03tEgkoBbjIUcjLg8cfh9GjWnEq3zGTGzir5mz+3RrebQqNftxBes90Wp3RyutSpQJTC0XkL5o/H84+292s7E4yKyo344srZtO0J2ScAUPnwlcvFCi8JeA0Ahc5Qrm5MGCAmxJ4xhkuyHcnDOaSmF/ZUBvar4KR6dAgF4iK8rpcCQM+jcCNMTWNMdOMMWuMMauNMS39VZhIMJk50y3ImTAB+veHDxZnMeb/2nF9q21EYEh/A6a9XRTeNWq4tfIiAeZrC2UMkGatbQKcA6z2vSSR4LF9O9x1F9x0k3syzoKP9nDCTYmcN6nY7JILJtGqIMo9aTgqyg3RO3XyunQJA0fdQjHGnABcAfwLwFq7F9jrn7JEvGUtvPWW2zkwNxeeeALOvS2VLnN7s37H+pKzSy4DOv/L44olHPkyAj8N2A68aoz5whgzwRhzTOmTjDHdjDHLjTHLt2/f7sPlRMrHd9/BLbfAnXfCaae5dsnKJu1o+/b1VDKVSO+cztSOUzU1UDznS4BXBs4DXrLWngv8BgwsfZK1NtlaG2Otjalbt64PlxMJLGvhlVegWTPIyIChz+3hxmcTuWVuycU4ml0iwcKXWShbgC3W2k+LXk+jjAAXCQUbN7rnUn74IVx5Jdz9ZCrDvurN+oXrtRhHgtZRj8CttT8Am40xjYsOXQOs8ktVIuVk/34YPdrN6162DBLHZlHzgXbcv0DtEgl+vs4DjwNSjDFVgY3Afb6XJFI+Vq1ym0998gm0vmEPZ3cfTtJXQzC5Rlu9SkjwKcCttSuBGD/VIlIu9u2DYcMgMRGOOw4eHp/GjPw40j9Xu0RCi1ZiSlhZsQK6dHFbv97YOYv918Xz3Ca31Wt6Z+1dIqFFe6FIWNi9GwYOhIsugm079nDXy4nMa9KMhVs0u0RCl0bgUuEtXux63ZmZ0LpnGpkN4/jfD2qXSOjTCFwqrF9+gZ494YorIC8yi0ufb0d63TZUrqzZJVIxaAQuFVJ6OnTrBjnf59PykeGsPC6Jnb9odolULApwqVB27ID4ePjvf6HeVWnU6xnH0t3r6dBI7RKpeBTgUmFMm+ZaJj8WZNN4UDxrK82gUY1GpLfX7BKpmBTgEvJ++MEF9/T38jmlw3CqNk9icyXD0CvULpGKTQEuIcta1yqJj4dfT07jxEFxbLXr6dBY7RIJDwpwCUnZ2dC9O6R/kk3tu+LZd9IMTqzdiP+1UbtEwoemEUpIKSyEsWOhWYt8PixIokp8U/bUS9diHAlLGoFLyFi7Fu6/Hz76IY0aPeLYV3097Zu2Z2TrkWqXSFjSCFyCXkGB23yqxeXZfBp9K3RuQ716bjHOtNumKbwlbGkELkFt5Uq479/5rKw+nIgeSVStanjqSs0uEQEFuASp/Hx4+mkYOjUNc31vqJnJLWqXiJSgAJegs3Qp3BOXzfqG8XDXDM6o2YgXb9TsEpHSFOASNH77DR5JyGfcF8MxbZKIrGoYfNUQ+rXsp3aJSBkU4BIU5s6FTk+kse283nB1Jjef2Z7nb1C7RORQFODiqV27oPsj2bydGw/XzaB+9UZM0N4lIkdEAS6emTYjny4TRvDLuYlUOcXw+BVDeORytUtEjpTPAW6MiQCWA99Za2/0vSSp6LZtg46PprPomDi4MJOrT27Pq3eoXSLyV/ljBN4HWA0c74ffJRWYtfD86zkMmBfP3obTOZFGvH57Ojc0UbtE5Gj4tBLTGFMPuAGY4J9ypKLakJ1P025D6Lu+CQWnpdK3+RC+S/hK4S3iA19H4KOBAcBxBzvBGNMN6AbQoIH+iRxuCguh79h0xm6Iw9bLpEXl9rzbYySn1dZ/CyK+OuoRuDHmRmCbtXbFoc6z1iZba2OstTF169Y92stJCFr0ZQ4n9W7PCztjqVYNXr0mjS8Tpvk3vFNSIDoaKlVyf6ek+O93iwQ5X0bglwI3G2OuB6oBxxtjJltrO/unNAlVefn5dBg5gtTfEqEmtDt+CP97rB/Vqvh5dklKintycV6ee52d7V4DdOrk32uJBCFjrfX9lxhzFdD/cLNQYmJi7PLly32+ngSv8XPT6ZMRR/4xmZyy07VLLmwcoHZJdLQL7dKioiArKzDXFPGAMWaFtTam9HHNAxe/WL89h5tfjGc106m070wG1ktjyKDWGBPAi+bk/LXjIhWMXwLcWrsAWOCP3yWhJb8gn/ipI3h5VSLWQotdQ5j1eD/qnVwOi3EaNCh7BK6b5RIm9EAHOTopKbzfsj6n9KvHS+sSiMxqxcRz1/Dl2EfLJ7wBkpKgRo2Sx2rUcMdFwoACXP6ynNfGcOXMfrSN3cJOahH7Rm9+eOdTuuxZXL6FdOoEycmu522M+zs5WTcwJWz45SbmkdJNzNCWX5BP0oIRDF3wJAVEUGtRD/63ZAWx+xe6E3TzUCQgdBNTfJK+Pp0u0+L4Pj8TMttxf3pTRueO5hjyDpykm4ci5UotFDmknNwcbnyjPbEpsXz/PTRYmMYniwp5JXdIyfAG3TwUKWcKcClTfkE+SYuGcOaYJsxam0ql+Uk8WvNr1qW25qLhHXXzUCQIqIUif5K+Pp0eH8Sx6edMWHUrzb8fScq4KFq0KDrh95uECQmubdKggQtv3TwUKVcKcPlDTm4O8WnxTF8znUo7z6RKRhpJ97UmPh4ql/4vpVMnBbaIxxTgQn5BPiOWjuDphYns3QssSKKleYhJ70bSqJHX1YnIwSjAw1z6+nTiUuPI3JFJxNpbqbZwJMMfj6J7d7fBn4gELwV4mMrJzSE+PZ7pq6dT7beGMCOV6xrGMn6pJpOIhAoFeJj5vV2SuCiRggKIWJBE9W8fInlEJJ07E9jNp0TEr/SP5FDjwwMM0tenc/ZLZ5PwYQJVc9qwb9Rq2tV5jNVfR3L33QpvkVCjEXgoOcoHGBRvl9QqbEilN1Op/nMskybBrbeWQ90iEhAagYeShIQD4f27vDx3vAz5BfkMWTyEJmPdYpw6XyaxM+kb7r00llWrFN4ioU4j8FDyFx5gUHx2yen57dj40ihOPiGKlNnQSg+CF6kQNAIPJQebHlLseE5uDu3fdnuX/JZn+VtGKpuGTSfu7ii++UbhLVKRKMBDySEeYFC8XZKamcq5OxP5PuEbav0Uy6JF8PzzcOyx3pQtIoGhFkooOcgeJOkX1SHupbPJ3JHJRce3Y+OLo/hqUxSPDoBBg6BaNW/LFpHA0Ag81HTq5B6aUFhIzpeLaF9lOrEpsRQUWC7LSuXTftM59dgoPvsMhgxReItUZBqBh6D8gnxGLh3J04ueBqBDrUTmJfbnu9xIkpLg4YehShWPixSRgDvqADfG1Af+C5wEWCDZWjvGX4VJ2YrPLmndoB35741i2ntRtGwJEydC06ZeVygi5cWXEXgB8JC19nNjzHHACmPMHGvtKj/VJsUUX4zTsHZDep6Qyuu9YykshDFjoGdPiIjwukoRKU9HHeDW2q3A1qKffzHGrAZOBRTgflS6XRLfIpFlY/ozbmEk117rHsJ+2mkeFykinvBLD9wYEw2cC3xaxnvdgG4ADbTN3V9SvF1yS+N2NM0exai7ooiMhAkToEsX7V8iEs58noVijDkWeAfoa639ufT71tpka22MtTambt26vl4uLOTk5tDh7Q7EpsRisbx4SSpbRkxn6MAoYmNh1Sro2lXhLRLufBqBG2Oq4MI7xVo73T8lha/f2yWJixOx1vLEFYnsXdCf3tdHUqsWvPUWdOyo4BYRx5dZKAaYCKy21o70X0nhKWNDBr1m9yJzRybtmrTjnpNGkdAzilWroHNnGD0aTjzR6ypFJJj40kK5FLgbuNoYs7Loz/V+qits/N4uaT25NRbLjA6pRH86nVuvjuLnn2HWLHjjDYW3iPyZL7NQPgL0j/mjVLpdkvjPRGL29efBmyPZuBEeeACeeQaOP97rSkUkWGkpvQcyNmRw9ktn89iHj9H6jNZ8es9qsicnEHttJJUqwYIF8NJLCm8ROTQtpS9HObk59Evvxzur36Fh7YakdkqlYE0sbVrC1q3Qvz88+eSfNxwUESmLArwclNUuubdRfwb0i2TKFGjeHGbMgAsu8LpSEQklCvAAy9iQQVxqHOt+Wke7Ju0Y2WoUn6RHcW57yM2FJ56ARx+FqlW9rlREQo0CPEDKapecXT2WHvfCBx/AhRe6zaeaN/e6UhEJVbqJ6Wf5BfkMXTyUpuOaMjtzNon/TOTrB75h8/xYmjWDuXNh+HBYskThLSK+0Qjcj0q3S0a1HsX+HVHcEAsffghXXQWvvAING3pdqYhUBBqB+0HxxTiFtpDUTqlM7TCd6a9G0bw5LFsG48fDvHkKbxHxH43AfVDW7JL+l/Rnw7pILrsMPvkEbrgBXn4Z6tXzuloRqWgU4EcqJaXEw4QzEu4gLn9GiXbJ34+JYthQSEyE445zH7nzTm0+JSKBoQA/Eikp0K0b5OWRcwL0uyibd75/hoZVTiK1UyqxDWNZsQJu7gJffQV33OGekvO3v3lduIhUZOqBH4mEBPbm5zHsMmjaE2afCYnz4JsJkVx5aiyPPOKmBW7fDu+9B1OmKLxFJPA0Aj8CGZWziesB6+pAu9UwKg2icmExUXQ9BzIz3QMWhg+HmjW9rlZEwoVG4Ifwx+ySu6HQQOpkmP4W1M49lp6M5QoWUVDg5nZPmKDwFpHypQAvw979exn20bADi3Fqd+Sb16oTux7SaM1ZfMtL9KBv7Gq+/hquucbrikUkHKmFUkpZi3Giakaxo+pUuj1Uif/+1p6mVTL5+LE5tHyitdflikgYU4AX2Zy7mX4Z/Zi2atofe5fENowFYNo06DmoIzvy3UzC//znTCIjz/S4YhEJd2Ef4Hv372Xk0pE8vejpEotxIitHsnUr9OoF06fDeedBRgacc47XFYuIOGEd4HM2zCEuNY61P60t0S6xFl57DeLjYfduGDYMHnoIKof1tyUiwSYsI6l0u2T2XbNpc2YbALKzoXt3SE+Hyy5zs0saN/a4YBGRMvg0C8UYE2uMWWuMWW+MGeivogLl99klTcY1Yda6WW6r1x5f0+bMNhQWwtixcNZZ8PHH7ueFCxXeIhK8jnoEboyJAMYB1wFbgGXGmPettav8VZw/HaxdArB2rVuI8/HH0Lq12zkwKsrjgkVEDsOXEfiFwHpr7UZr7V7gTaCtf8ryn825m+k4tSOtJrdiv93P7LtmM/326UTVjGLfPtffPuccWLXK9b1TUxXeIhIafOmBnwpsLvZ6C3BR6ZOMMd2AbgANGjTw4XJ/TVmzSx665CGqVa4GwMqV0KULfPEFtG/vWiYnn1xu5YmI+CzgNzGttclAMkBMTIwN9PWgZLvklia3MKr1KKJrRgOwZ4/b7vWZZ+DEE90c7/bty6MqERH/8iXAvwPqF3tdr+iYZw41uwTccyi7doU1a+Dee2HkSKhd28OCRUR84EuALwPONMachgvuO4C7/FLVX7R3/15GLR3FU4ueKrNd8uuvbgXlCy9A/fqQluZuVoqIhLKjDnBrbYExpheQDkQAk6y13/qtsiN0qHYJuJ0C//1vyMqCnmi9zM0AAASPSURBVD1h6FD3tBwRkVDnUw/cWjsbmO2nWv6S4u2SM2qd8ad2ya5dbvXkpEnQqBEsWgSXX+5FpSIigRFyKzFLt0ue/ufT9L+k/x/tEoB334UHH4Rt22DgQBg0CKpX97BoEZEACKkAn7txLr1m9zpou2TbNoiLg7ffdnO7P/gAzj/fu3pFRAIpJAJ8155ddPugG1NXTS2zXWKte+5wnz7uhmViIgwYAFWqeFi0iEiAhUSAH1v1WLJzs8tsl2zeDA88ALNnw8UXw8SJ0KyZh8WKiJSTkAjwypUqs7TrUiqZAyv/CwshOdmNtPfvh9Gj3d7dEREeFioiUo5CIsCBEuGdmemmBi5c6J5HmZwMp5/uYXEiIh4IjYcap6RAdDQFpgrDayXR4qwCVq50e3XPmaPwFpHwFPwj8JQU6NaNr/NOpysfs2zXhdwcMZOXEvfy9663el2diIhngn8EnpDAj3nVuYhPySKaN7mdd/ffxN+H9/O6MhERTwX/CDwnhzpYXude/sl86vDTH8dFRMJZ8I/Ai/YQ78i0A+Fd7LiISLgK/gBPSoIaNUoeq1HDHRcRCWPBH+CdOrl5glFRYIz7OznZHRcRCWPB3wMHF9YKbBGREoJ/BC4iImVSgIuIhCgFuIhIiFKAi4iEKAW4iEiIMtba8ruYMduBbB9+RR3gRz+VE+r0XZSk7+MAfRcHVJTvIspaW7f0wXINcF8ZY5Zba2O8riMY6LsoSd/HAfouDqjo34VaKCIiIUoBLiISokItwJO9LiCI6LsoSd/HAfouDqjQ30VI9cBFROSAUBuBi4hIEQW4iEiICokAN8bEGmPWGmPWG2MGel2Pl4wxk4wx24wx33hdi9eMMfWNMfONMauMMd8aY/p4XZNXjDHVjDGfGWO+LPounvS6pmBgjIkwxnxhjJnpdS2BEPQBboyJAMYBbYBmwJ3GmGbeVuWp14BYr4sIEgXAQ9baZsDFQM8w/m8jH7jaWnsO8A8g1hhzscc1BYM+wGqviwiUoA9w4EJgvbV2o7V2L/Am0NbjmjxjrV0E7PC6jmBgrd1qrf286OdfcP+jnuptVd6wzq9FL6sU/QnrGQrGmHrADcAEr2sJlFAI8FOBzcVebyFM/yeVgzPGRAPnAp96W4l3itoFK4FtwBxrbdh+F0VGAwOAQq8LCZRQCHCRQzLGHAu8A/S11v7sdT1esdbut9b+A6gHXGiMae51TV4xxtwIbLPWrvC6lkAKhQD/Dqhf7HW9omMiGGOq4MI7xVo73et6goG1dhcwn/C+V3IpcLMxJgvXdr3aGDPZ25L8LxQCfBlwpjHmNGNMVeAO4H2Pa5IgYIwxwERgtbV2pNf1eMkYU9cYU7Po5+rAdcAab6vyjrX2UWttPWttNC4zPrTWdva4LL8L+gC31hYAvYB03E2qt62133pblXeMMVOApUBjY8wWY0xXr2vy0KXA3bjR1cqiP9d7XZRHTgHmG2O+wg165lhrK+TUOTlAS+lFREJU0I/ARUSkbApwEZEQpQAXEQlRCnARkRClABcRCVEKcBGREKUAFxEJUf8PeKVkR74/A+IAAAAASUVORK5CYII=
"/></p>
<p>最后，我们重新用上一遍笔记的梯度下降法计算 <span class="math">\(y = ax + b\)</span> 的参数 <span class="math">\(a\)</span> 和 <span class="math">\(b\)</span>。我们使用跟 <code>model</code> 一样的参数初值，通过同样100次迭代得到了跟 <code>model.parameters</code> 一样的参数，这从另一个侧面验证我们的实现是正确的。</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">,</span> <span class="n">sxy</span><span class="p">,</span> <span class="n">sx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">ga</span> <span class="o">=</span> <span class="p">(</span><span class="n">sx2</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="n">sx</span> <span class="o">*</span> <span class="n">b</span> <span class="o">-</span> <span class="n">sxy</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">gb</span> <span class="o">=</span> <span class="p">(</span><span class="n">sx</span> <span class="o">*</span> <span class="n">a</span> <span class="o">-</span> <span class="n">sy</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">a</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">ga</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">gb</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;a: {a:.8f}, b: {b:.8f}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>


<pre>a: 2.76579559, b: -0.62210507
{'W': array([[2.76579559]]), 'b': array([-0.62210507])}
</pre>

        <div class="tags">
        <a href="/tag/python">python</a>
        <a href="/tag/numpy">numpy</a>
        <a href="/tag/npnet">npnet</a>
        </div>


<div id="disqus_thread">
  <div class="btn_click_load">
    <button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button>
  </div>
  <script>
    var disqus_shortname = 'wormtooth';
    var disqus_identifier = 'npnet1/';
    var disqus_title = '[npnet] 线性回归';
    var disqus_url = 'https://wormtooth.com/npnet1/';
    $('.btn_click_load').click(function() {
      (function() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || 
          document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      $('.btn_click_load').css('display','none');
    });

    $.ajax({
      url: 'https://disqus.com/favicon.ico',
      timeout: 3000,
      type: 'GET',
      success: (function() {
        var dsq = document.createElement('script'); 
        dsq.type = 'text/javascript'; 
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || 
          document.getElementsByTagName('body')[0]).appendChild(dsq);
        $('.btn_click_load').css('display','none');
      })(),
      error: function() {
        $('.btn_click_load').css('display','block');
        }
      });
  </script>
  <script id="dsq-count-scr" src="//wormtooth.disqus.com/count.js" async></script>
</div>
    </div>
</div>
        </div></div>
        <div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar">
<div class="widget">
    <form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form">
        <input type="text" name="q" maxlength="20" placeholder="Search"/>
        <input type="hidden" name="sitesearch" value="https://wormtooth.com"/>
    </form>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-heart-o"> 年轻的心只有一面</i></div>
    <img src="/images/mobius_heart.png" class="nofancybox" />
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-paper-plane-o"> 心情随笔</i></div>
    <p>含蓄的极致是闷骚，闷骚的极致是深情。</p>
    <span class="qed"><a href="/scribble">View All</a></span>
</div><div class="widget">
    <div class="widget-title">
        <i class="fa fa-folder-o"> Categories</i>
    </div>
    <ul class="category-list">
        <li class="category-list-item"><a class="category-list-link" href="/category/life/">Life</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/machine-learning/">Machine Learning</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/mathematics/">Mathematics</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/notes/">Notes</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/programming/">Programming</a></li>
    </ul>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div>
    <ul></ul>
    <a href="https://yongjiasong.com/" title="JOY DOMAIN" target="_blank">JOY DOMAIN</a>
</div>
<div class="widget">
    <div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div>
    <script type="text/javascript" src="//wormtooth.disqus.com/recent_comments_widget.js?num_items=5&hide_avatars=1&avatar_size=32&excerpt_length=20&hide_mods=1"></script>
</div>
        </div></div>
        <div class="pure-u-1 pure-u-lg-3-4">
<div id="footer">Copyright © 2019 <a href="/." rel="nofollow">叶某人的碎碎念.</a> <a rel="nofollow" target="_blank" href="https://getpelican.com/">Pelican</a> &amp; <a rel="nofollow", target="_blank", href="https://github.com/wormtooth/maupassant-pelican">maupassant</a>.</div>        </div>
    </div>
</div>
<a id="rocket" href="#top" class="show"></a>
<script type="text/javascript" src="/theme/js/totop.js" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script>
<script type="text/javascript" src="/theme/js/fancybox.js" async></script>
<link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css" />

<script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      },
      Macros: {
        N: "\\mathbb{N}",
        Z: "\\mathbb{Z}",
        Q: "\\mathbb{Q}",
        R: "\\mathbb{R}",
        C: "\\mathbb{C}"
      }
    },
    'HTML-CSS': {
      imageFont: null
    }
  });
</script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.1.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>