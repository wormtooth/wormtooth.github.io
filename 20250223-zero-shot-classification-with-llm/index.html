<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport" />
    <meta content="yes" name="apple-mobile-web-app-capable" />
    <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style" />
    <meta content="telephone=no" name="format-detection" />
    <title>
Zero-Shot Text Classification with pretrained LLM | Âè∂Êüê‰∫∫ÁöÑÁ¢éÁ¢éÂøµ    </title>
    <link rel="stylesheet" type="text/css" href="/theme/css/style.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/pygment.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/normalize/6.0.0/normalize.min.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/pure-min.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/grids-responsive-min.css" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7674955363445536"
     crossorigin="anonymous"></script>
</head>

<body>
<div class="body_container">
    <div id="header">
        <div class="site-name">
            <a id="logo" href="/."> Âè∂Êüê‰∫∫ÁöÑÁ¢éÁ¢éÂøµ </a>
            <p class="description"> Believe in Mathematics </p>
        </div>
        <div id="nav-menu">
            <a href="/."><i class="fa fa-home"> Home</i></a>
            <a href="/movies/"><i class="fa fa-film"> Movies</i></a>
            <a href="/archives.html"><i class="fa fa-archive"> Archive</i></a>
        </div>
    </div>
    <div id="layout" class="pure-g">
        <div class="pure-u-1 pure-u-lg-3-4"><div class="content_container">
<div class="post">
    <h1 class="post-title">Zero-Shot Text Classification with pretrained LLM</h1>
    <div class="post-meta">Feb 23, 2025
    <span> | </span> <span>Machine Learning</span>
    </div>
    <a data-disqus-identifier="20250223-zero-shot-classification-with-llm/" href="/20250223-zero-shot-classification-with-llm/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p>According to this <a href="https://huggingface.co/tasks/zero-shot-classification">article</a>,</p>
<blockquote>
<p>Zero-shot text classification is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.</p>
</blockquote>
<p>Simply put, zero-shot text classification is to use preexisting models on classification tasks that the models are not trained upon. Large Language Models backed by <a href="https://arxiv.org/abs/1706.03762">attention</a> have a lot of great applications, such as summarization, chatbot, code completion and etc. It aslo gives zero-shot text classification a huge potential since most LLMs are pretrained on tremendous data which cover most common use case already. LLMs with strong reasoning capability such as deepseek can even perform well on unseen data. In this article, I want to discuss some pratical ways to use pretrained LLMs to do zero-shot classification using <a href="https://huggingface.co/docs/transformers/en/index">ü§ó Transformers</a>. <br>
<!--more--></p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span>
    <span class="n">TextClassificationPipeline</span><span class="p">,</span>
    <span class="n">TextGenerationPipeline</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>


<p>There has been a lot of research on zero-shot classification, for example, <a href="https://arxiv.org/abs/2104.08315">[1]</a>, <a href="https://arxiv.org/abs/2404.11122">[2]</a> and etc. Given a set of labels <span class="math">\({y_i}\)</span>, and a sample <span class="math">\(x\)</span>, we will implement the basic method<br>
</p>
<div class="math">$$\mathrm{arg max}_i P(y_i|x).$$</div>
<p><br>
As an example, we will use a sentiment dataset <a href="https://huggingface.co/datasets/vumichien/financial-sentiment">financial-sentiment</a>. In this dataset, we need to classify some finanical text into 3 labels - negative, netural and positive.</p>
<div class="highlight"><pre><span></span><span class="c1"># https://huggingface.co/datasets/vumichien/financial-sentiment</span>
<span class="c1"># use the valid split - </span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s2">&quot;hf://datasets/vumichien/financial-sentiment/data/valid-00000-of-00001.parquet&quot;</span>
<span class="n">df_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
<span class="n">df_data</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;label_experts&quot;</span><span class="p">:</span> <span class="s2">&quot;label&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: </span><span class="si">{</span><span class="n">df_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Labels: </span><span class="si">{</span><span class="n">df_data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
    <span class="n">label_size</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label sample size for </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">label_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>Sample size: 453
Labels: ['negative', 'neutral', 'positive']
Label sample size for negative: 61
Label sample size for neutral: 265
Label sample size for positive: 127
</pre>

<p>There are roughly two types of pretrained models, base models and instruct models. Base models are pretrained only on a large corpus of text. Instruct models are further trained on instructions to give capability to models to follow instructions. I am going to use <a href="https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e">Qwen2.5</a>-family models as there are base and instruct models. For simplicity, I will use <a href="https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct">Qwen/Qwen2.5-0.5B-Instruct</a> throughout the article. In theory, one can use any completion models, base or instruct. Personally, I prefer instruct models as they can follow instructions better, as you can already tell from "Instruct" in their names.</p>
<h2>Instruct Model as Text Classifier</h2>
<p>GPT models are generally pretrained on predicting the next token. The idea here to use LLM as a text classifier is to create a prompt so that the next token from LLM is limited to the labels of the classification tasks. For our example with financial sentiment ananlysis, we should create a prompt so that the LLM output negative, netrual, or positive. Here is one possible prompt:</p>
<div class="highlight"><pre><span></span>What is the sentiment of the following text related to finance?
negative, neutral or positive: {text}
Give your answer in one word.
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">lm_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>
</pre></div>


<p>Let's get our prompt template.</p>
<div class="highlight"><pre><span></span><span class="n">user_prompt_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;What is the sentiment of the following text related to finance?</span>
<span class="s2">negative, neutral or positive: </span><span class="si">{text}</span>
<span class="s2">Give your answer in one word.&quot;&quot;&quot;</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_prompt_template</span><span class="p">}</span>
<span class="p">]</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">)</span>
</pre></div>


<pre><|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
What is the sentiment of the following text related to finance?
negative, neutral or positive: {text}
Give your answer in one word.<|im_end|>
<|im_start|>assistant

</pre>

<div class="highlight"><pre><span></span><span class="c1"># apply the prompt template to all samples</span>
<span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">text</span><span class="p">:</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">))</span>
</pre></div>


<p>We have our prompt template ready. There is a place holder <code>{text}</code> where we put in our text to be classified. Let's run it on one text sample.</p>
<div class="highlight"><pre><span></span><span class="n">text_sample</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text_sample</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentiment (Truth): </span><span class="si">{</span><span class="n">text_sample</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>In Q2 of 2009 , profit before taxes amounted to EUR 13.6 mn , down from EUR 26.8 mn in Q2 of 2008 .
Sentiment (Truth): negative
</pre>

<div class="highlight"><pre><span></span><span class="c1"># the prompt we will send to the model</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">text_sample</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>


<pre><|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
What is the sentiment of the following text related to finance?
negative, neutral or positive: In Q2 of 2009 , profit before taxes amounted to EUR 13.6 mn , down from EUR 26.8 mn in Q2 of 2008 .
Give your answer in one word.<|im_end|>
<|im_start|>assistant

</pre>

<div class="highlight"><pre><span></span><span class="c1"># get the prediction from the model</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">lm_model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="c1"># step 1</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">lm_model_output</span> <span class="o">=</span> <span class="n">lm_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span> <span class="c1"># step 2</span>

<span class="n">predicted_token_id</span> <span class="o">=</span> <span class="n">lm_model_output</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span> <span class="c1"># step 3</span>
<span class="n">predicted_label</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">predicted_token_id</span><span class="p">)</span> <span class="c1"># step 4</span>

<span class="nb">print</span><span class="p">(</span><span class="n">text_sample</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentiment (Truth): </span><span class="si">{</span><span class="n">text_sample</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentiment (Predicted): </span><span class="si">{</span><span class="n">predicted_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>In Q2 of 2009 , profit before taxes amounted to EUR 13.6 mn , down from EUR 26.8 mn in Q2 of 2008 .
Sentiment (Truth): negative
Sentiment (Predicted): negative
</pre>

<p>Working great! We get the correct label for the sample using the model. But how does it work? Let's break it down step by step.</p>
<p>Step 1. Tokenize the text input into numbers that can be accepted by the model.</p>
<p>Step 2. Run the model to get the prediction of next tokens. Note that we don't just get one predicted tokens, but a list of tokens. For example, if the text input is "This is an input sentence", then roughly the model doing the following predictions:<br>
- This -&gt; is<br>
- This is -&gt; an<br>
- This is an -&gt; input<br>
- This is an input -&gt; sentence<br>
- This is an input sentence -&gt; ?</p>
<p>So only the last prediction is interesting to us, as this is the token that continues the whole input seqeunce.</p>
<p>Step 3. Get the predicted token id for the last prediction. </p>
<p>Step 4. Convert the token id to word.</p>
<p>Now, we will show how to run the model of all samples more efficiently.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">classify_with_lm</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the prediction for a list of texts.</span>

<span class="sd">    For faster inference in a memory efficent way, we will do batch scoring.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
    <span class="n">texts_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
    <span class="c1"># batch runs</span>
    <span class="k">for</span> <span class="n">batch_begin</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">texts_count</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_end</span> <span class="o">=</span> <span class="n">batch_begin</span> <span class="o">+</span> <span class="n">batch_size</span>
        <span class="n">batch_texts</span> <span class="o">=</span> <span class="n">texts</span><span class="p">[</span><span class="n">batch_begin</span><span class="p">:</span> <span class="n">batch_end</span><span class="p">]</span>
        <span class="c1"># tokenize batch of texts</span>
        <span class="n">batch_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">batch_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># scoring</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">batch_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch_inputs</span><span class="p">)</span>
        <span class="c1"># get the last predicted tokens</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">batch_outputs</span><span class="o">.</span><span class="n">logits</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">batch_inputs</span><span class="o">.</span><span class="n">input_ids</span>
        <span class="c1"># since inputs has different length, they are padded</span>
        <span class="c1"># we want the last non pad location</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">last_non_pad_token</span> <span class="o">=</span> <span class="p">(</span><span class="n">token_indices</span> <span class="o">*</span> <span class="n">non_pad_mask</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># get logits of last non pad token</span>
        <span class="n">pooled_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch_texts</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">last_non_pad_token</span><span class="p">]</span>
        <span class="c1"># get the predicted token ids</span>
        <span class="n">predicted_token_ids</span> <span class="o">=</span> <span class="n">pooled_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># convert token ids back to tokens</span>
        <span class="n">predicted_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">predicted_token_ids</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">predicted_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>


<p>The above function <code>classify_with_lm</code> does batch inference instead for efficiency. In order to do batch inference, the inputs must be padded to the same length. That is the reason for <code>non_pad_mask</code> to figure out the last non pad token.</p>
<div class="highlight"><pre><span></span><span class="n">lm_results</span> <span class="o">=</span> <span class="n">classify_with_lm</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">lm_model</span><span class="p">,</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The set of predicted labels: </span><span class="si">{</span><span class="nb">set</span><span class="p">(</span><span class="n">lm_results</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">],</span> <span class="n">lm_results</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="n">label</span> <span class="o">==</span> <span class="n">prediction</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correct count = </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">lm_results</span><span class="p">)</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">lm_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>The set of predicted labels: {'negative', 'Positive', 'Negative', 'Neutral', 'positive', 'neutral'}
Correct count = 323/453 = 71.30%
</pre>

<p>We get it working, and the accuracy is not bad. But we immediately notice a problem: the predicted labels "Negative" and "Positive" are not exactly the wanted labels. In our examples, they are easy to handle. We just need to make them lower cases. However, there is still a risk that the predicted label is random.</p>
<div class="highlight"><pre><span></span><span class="c1"># accuracy when we convert the labels to lower cases.</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">],</span> <span class="n">lm_results</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="n">label</span> <span class="o">==</span> <span class="n">prediction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correct count = </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">lm_results</span><span class="p">)</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">lm_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>Correct count = 350/453 = 77.26%
</pre>

<p>Before I end the discussion with using <code>CausalLM</code> as classifier, I want to point out that we can use <code>TextGenerationPipeline</code> to replace <code>classify_with_lm</code>. But keep in mind, <code>TextGenerationPipeline</code> requires padding the sequence on the left side for batch inference.</p>
<div class="highlight"><pre><span></span><span class="c1"># load a tokenizer with left padding</span>
<span class="n">lm_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span>
    <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span> <span class="c1"># NOTE: must set to left for batch inference in TextGenerationPipeline</span>
<span class="p">)</span>
<span class="c1"># set up the pipeline</span>
<span class="n">lm_pipe</span> <span class="o">=</span> <span class="n">TextGenerationPipeline</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">lm_tokenizer</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">lm_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># run on a sample</span>
<span class="n">lm_pipe_result</span> <span class="o">=</span> <span class="n">lm_pipe</span><span class="p">(</span>
    <span class="n">text_sample</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span>
    <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># we just need the output token</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># we only want one token in the output</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># we want the token with maximal probability</span>
    <span class="c1"># unset the following arguments to avoid warning</span>
    <span class="n">repetition_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted label from lm_pipe: </span><span class="si">{</span><span class="n">lm_pipe_result</span><span class="p">[</span><span class="s1">&#39;generated_text&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>Device set to use mps
</pre>

<pre>Predicted label from lm_pipe: negative
</pre>

<div class="highlight"><pre><span></span><span class="c1"># run the pipeline on all samples</span>
<span class="n">lm_pipe_results</span> <span class="o">=</span> <span class="n">lm_pipe</span><span class="p">(</span>
    <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># we just need the output token</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># we only want one token in the output</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># we want the token with maximal probability</span>
    <span class="c1"># unset the following arguments to avoid warning and discrepancy</span>
    <span class="n">repetition_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>

<span class="n">lm_pipe_results_clean</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">lm_pipe_results</span>
<span class="p">]</span>

<span class="c1"># accuracy with TextGenerationPipeline</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">],</span> <span class="n">lm_pipe_results_clean</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="n">label</span> <span class="o">==</span> <span class="n">prediction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correct count = </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">lm_results</span><span class="p">)</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">lm_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>Correct count = 352/453 = 77.70%
</pre>

<p>There are some slight discrepancy between the results from <code>classifiy_with_lm</code> vs <code>TextGenerationPipeline</code>. The reason is that we do right padding in <code>classify_with_lm</code> but left padding in <code>TextGenerationPipeline</code>. Please note that we make <code>do_sample</code> <code>False</code> since we want the token with maximal probability instead of sampling. We unset <code>temperature</code>, <code>top_k</code> and <code>top_p</code> to be compatible with <code>do_sample=False</code>. We also unset <code>repetition_penalty</code> to get minimal discrepancy as we didn't do repetition penalty in <code>classify_with_lm</code> either.</p>
<h2>Load Models as Sequence Classifier</h2>
<p>To address the problem to avoid random output, we can force the output tokens to the labels. The idea is to ignore logits of all tokens but the labels. We can simply modify the <code>classify_with_lm</code> function above to focus on only logits of the labels. I will go one step forward to convert the model from <code>CausalLM</code> to <code>SequenceClassification</code>.</p>
<div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">,</span> <span class="s2">&quot;negative&quot;</span><span class="p">,</span> <span class="s2">&quot;neutral&quot;</span><span class="p">]</span>
<span class="n">labels_token_ids</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">label</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">label_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">labels_token_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The label `</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">` has token id: </span><span class="si">{</span><span class="n">label_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>The label `positive` has token id: 30487
The label `negative` has token id: 42224
The label `neutral` has token id: 59568
</pre>

<p>The order of the labels is chosen so that the token ids form an ascending order. This is an important details if we want to have a consistent result with <code>CausalLM</code>. I will leave it as an exercise for you to figure out why it matters - A hint for you is the behavior of <code>torch.argmax</code> when there are multiple maximums.</p>
<div class="highlight"><pre><span></span><span class="c1"># load classification model</span>
<span class="n">cls_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># extract the weights for the labels</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">lm_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">cls_weights</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;lm_head.weight&quot;</span><span class="p">][</span><span class="n">labels_token_ids</span><span class="p">]</span>

<span class="c1"># load them into the scoring module</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">cls_model</span><span class="o">.</span><span class="n">score</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">cls_weights</span><span class="p">)</span>
</pre></div>


<pre>Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>

<p>Okay, what is going on here? The LLM models are classification models as well - they predict the labels for the next tokens. Now for our sentiment analysis, we only need to know their predictions on a limited set of 3 tokens: negative, neutral and positive. So we can extract the weights to calculate the logits for the labels and use them to initialize a <code>SequenceClassification</code> model. Let's see how to use it for a sample.</p>
<div class="highlight"><pre><span></span><span class="c1"># get the prediction from the model</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cls_model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">cls_model_output</span> <span class="o">=</span> <span class="n">cls_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>

<span class="n">label_id</span> <span class="o">=</span> <span class="n">cls_model_output</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">predicted_label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">label_id</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">text_sample</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentiment (Truth): </span><span class="si">{</span><span class="n">text_sample</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentiment (Predicted): </span><span class="si">{</span><span class="n">predicted_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>In Q2 of 2009 , profit before taxes amounted to EUR 13.6 mn , down from EUR 26.8 mn in Q2 of 2008 .
Sentiment (Truth): negative
Sentiment (Predicted): negative
</pre>

<p>One of the good things using <code>SequenceClassification</code> is that we can assemble a <code>TextClassificationPipeline</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># set the labels for the classification model</span>
<span class="n">labels_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="n">label2id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="n">labels_count</span><span class="p">)))</span>
<span class="n">id2label</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">labels_count</span><span class="p">),</span> <span class="n">labels</span><span class="p">))</span>
<span class="n">cls_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">label2id</span> <span class="o">=</span> <span class="n">label2id</span>
<span class="n">cls_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="n">id2label</span>
</pre></div>


<p>We set the <code>label2id</code> and <code>id2label</code> so that <code>TextClassificationPipeline</code> can return the correct labels.</p>
<div class="highlight"><pre><span></span><span class="c1"># set the padding token so that we can run the pipeline in batches</span>
<span class="n">cls_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">TextClassificationPipeline</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">cls_model</span>
<span class="p">)</span>

<span class="n">pipe</span><span class="p">(</span><span class="n">text_sample</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">])</span>
</pre></div>


<pre>Device set to use mps
</pre>

<pre>[{'label': 'negative', 'score': 0.6845569014549255}]</pre>

<div class="highlight"><pre><span></span><span class="n">cls_results</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">(),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">result</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cls_results</span><span class="p">,</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]):</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">label</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correct count = </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cls_results</span><span class="p">)</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">cls_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>Correct count = 351/453 = 77.48%
</pre>

<p>We set the batch size = 32 in the pipeline to be consistent with what we used before. Setting it to other number might affect the results a bit. The reason is that the number of pading tokens has some impact of logits, since the embedding of the padding token is non-zero. We almost reproduce the same result as using <code>CausalLM</code> above, except that we get one more sample right.</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">result1</span><span class="p">,</span> <span class="n">result2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">lm_results</span><span class="p">,</span> <span class="n">cls_results</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">result1</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">!=</span> <span class="n">result2</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LM prediction: </span><span class="si">{</span><span class="n">result1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CLS prediction: </span><span class="si">{</span><span class="n">result2</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>


<pre>LM prediction: Neutral
CLS prediction: positive
</pre>

<p>Since we only allow "positive", "negative" and "neutral" in <code>SequenceClassifier</code>, it does change the behavior if <code>CausalLM</code> outputs other labels than the 3 above.</p>
<h2>Prompt Matters</h2>
<p>In the zero-shot classification, the input prompt matters, as it is the main way we can change the performance of the pretrained LLM towards our tasks. To demonstrate the effect, let's run another prompt with a task given in the system message.</p>
<div class="highlight"><pre><span></span><span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&quot;You are a helpful assistant that good at sentiment analysis on texts related to finance.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span class="n">prompt2</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;What is the sentiment of the following text related to finance?</span>
<span class="s2">negative, neutral or positive: </span><span class="si">{text}</span>
<span class="s2">Give your answer in one word.&quot;&quot;&quot;</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt2</span><span class="p">}</span>
<span class="p">]</span>
<span class="n">prompt_template2</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt_template2</span><span class="p">)</span>
</pre></div>


<pre><|im_start|>system
"You are a helpful assistant that good at sentiment analysis on texts related to finance.<|im_end|>
<|im_start|>user
What is the sentiment of the following text related to finance?
negative, neutral or positive: {text}
Give your answer in one word.<|im_end|>
<|im_start|>assistant

</pre>

<div class="highlight"><pre><span></span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">text</span><span class="p">:</span> <span class="n">prompt_template2</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">))</span>
<span class="n">cls_results2</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt2&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">(),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">result</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cls_results2</span><span class="p">,</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]):</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">label</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correct count = </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cls_results2</span><span class="p">)</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">cls_results2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>


<pre>Correct count = 374/453 = 82.56%
</pre>

<p>With a simple update in the system message, we gain 5% in performance! We can keep doing prompt engineering following <a href="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api">best practices</a>. But it is not efficient. A more efficient way is to do fine-tuning with some data. Since our focus is zero-shot classification, I will leave the discussion out. But one of the advantages to set up <code>SequenceClassifier</code> is that it is readily for fine-tuning as a classifier with task specific data.</p>

        <div class="tags">
        <a href="/tag/classification">classification</a>
        <a href="/tag/llm">LLM</a>
        </div>

        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7674955363445536"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-7674955363445536"
     data-ad-slot="4714691501"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<div id="disqus_thread">
  <div class="btn_click_load">
    <button class="disqus_click_btn">ÈòÖËØªËØÑËÆ∫ „ÄåËØ∑Á°Æ‰øù disqus.com ÂèØ‰ª•Ê≠£Â∏∏Âä†ËΩΩ„Äç</button>
  </div>
  <script>
    var disqus_shortname = 'wormtooth';
    var disqus_identifier = '20250223-zero-shot-classification-with-llm/';
    var disqus_title = 'Zero-Shot Text Classification with pretrained LLM';
    var disqus_url = 'https://wormtooth.com/20250223-zero-shot-classification-with-llm/';
    $('.btn_click_load').click(function() {
      (function() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || 
          document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      $('.btn_click_load').css('display','none');
    });

    $.ajax({
      url: 'https://disqus.com/favicon.ico',
      timeout: 3000,
      type: 'GET',
      success: (function() {
        var dsq = document.createElement('script'); 
        dsq.type = 'text/javascript'; 
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || 
          document.getElementsByTagName('body')[0]).appendChild(dsq);
        $('.btn_click_load').css('display','none');
      })(),
      error: function() {
        $('.btn_click_load').css('display','block');
        }
      });
  </script>
  <script id="dsq-count-scr" src="//wormtooth.disqus.com/count.js" async></script>
</div>
    </div>
</div>
        </div></div>
        <div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar">
<div class="widget">
    <form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form">
        <input type="text" name="q" maxlength="20" placeholder="Search"/>
        <input type="hidden" name="sitesearch" value="https://wormtooth.com"/>
    </form>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-heart-o"> Âπ¥ËΩªÁöÑÂøÉÂè™Êúâ‰∏ÄÈù¢</i></div>
    <img src="/images/mobius_heart.png" class="nofancybox" />
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-paper-plane-o"> ÂøÉÊÉÖÈöèÁ¨î</i></div>
    <p>Êú™Êù•‰ºöÊù•ÔΩû</p>
    <span class="qed"><a href="/scribble">View All</a></span>
</div><div class="widget">
    <div class="widget-title">
        <i class="fa fa-folder-o"> Categories</i>
    </div>
    <ul class="category-list">
        <li class="category-list-item"><a class="category-list-link" href="/category/life/">Life</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/machine-learning/">Machine Learning</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/mathematics/">Mathematics</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/notes/">Notes</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/programming/">Programming</a></li>
    </ul>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div>
    <ul></ul>
    <a href="https://yongjiasong.com/" title="JOY DOMAIN" target="_blank">JOY DOMAIN</a>
</div>
<div class="widget">
    <div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div>
    <script type="text/javascript" src="//wormtooth.disqus.com/recent_comments_widget.js?num_items=5&hide_avatars=1&avatar_size=32&excerpt_length=20&hide_mods=1"></script>
</div>
        </div></div>
        <div class="pure-u-1 pure-u-lg-3-4">
<div id="footer">Copyright ¬© 2025 <a href="/." rel="nofollow">Âè∂Êüê‰∫∫ÁöÑÁ¢éÁ¢éÂøµ.</a> <a rel="nofollow" target="_blank" href="https://getpelican.com/">Pelican</a> &amp; <a rel="nofollow", target="_blank", href="https://github.com/wormtooth/maupassant-pelican">maupassant</a>.</div>        </div>
    </div>
</div>
<a id="rocket" href="#top" class="show"></a>
<script type="text/javascript" src="/theme/js/totop.js" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.js" async></script>
<script type="text/javascript" src="/theme/js/fancybox.js" async></script>
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.css" />

<script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      },
      Macros: {
        N: "\\mathbb{N}",
        Z: "\\mathbb{Z}",
        Q: "\\mathbb{Q}",
        R: "\\mathbb{R}",
        C: "\\mathbb{C}"
      }
    },
    'HTML-CSS': {
      imageFont: null
    }
  });
</script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.1.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>