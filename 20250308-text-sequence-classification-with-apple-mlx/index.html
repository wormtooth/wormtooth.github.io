<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport" />
    <meta content="yes" name="apple-mobile-web-app-capable" />
    <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style" />
    <meta content="telephone=no" name="format-detection" />
    <title>
Sequence Classification with Apple MLX | 叶某人的碎碎念    </title>
    <link rel="stylesheet" type="text/css" href="/theme/css/style.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/pygment.css" />
    <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/normalize/6.0.0/normalize.min.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/pure-min.css" />
    <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/grids-responsive-min.css" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7674955363445536"
     crossorigin="anonymous"></script>
</head>

<body>
<div class="body_container">
    <div id="header">
        <div class="site-name">
            <a id="logo" href="/."> 叶某人的碎碎念 </a>
            <p class="description"> Believe in Mathematics </p>
        </div>
        <div id="nav-menu">
            <a href="/."><i class="fa fa-home"> Home</i></a>
            <a href="/movies/"><i class="fa fa-film"> Movies</i></a>
            <a href="/archives.html"><i class="fa fa-archive"> Archive</i></a>
        </div>
    </div>
    <div id="layout" class="pure-g">
        <div class="pure-u-1 pure-u-lg-3-4"><div class="content_container">
<div class="post">
    <h1 class="post-title">Sequence Classification with Apple MLX</h1>
    <div class="post-meta">Mar 08, 2025
    <span> | </span> <span>Machine Learning</span>
    </div>
    <a data-disqus-identifier="20250308-text-sequence-classification-with-apple-mlx/" href="/20250308-text-sequence-classification-with-apple-mlx/#disqus_thread" class="disqus-comment-count"></a>
    <div class="post-content">
        <p><a href="https://github.com/ml-explore/mlx">MLX</a> is an array framework for machine learning on Apple silicon. The biggest advantage of the framework is the compatibility with the unified memory on Apple so that operations on MLX arrays can be performed on any of the supported device types without transferring data. It makes MLX a strong candidate when it comes to inferencing and even training a large model on Apple silicon. There are examples specifically designed for <a href="https://github.com/ml-explore/mlx-examples/tree/main/llms">LLM</a>, with a focus on text completion. As of today, there are few examples on other LLM tasks such as sequence classification for MLX since the framework is relatively new. I will provide an example to do classification inference with MLX, replicating what I did in my previous article <a href="https://wormtooth.com/20250223-zero-shot-classification-with-llm/">Zero-Shot Text Classification with pretrained LLM</a>.</p>
<!--more>

## Setup

We will need both `mlx` and `mlx-lm`:


<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mlx<span class="w"> </span>mlx-lm
</pre></div>




<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">asdict</span>

<span class="c1"># MLX imports</span>
<span class="kn">import</span> <span class="nn">mlx.core</span> <span class="k">as</span> <span class="nn">mx</span>
<span class="kn">import</span> <span class="nn">mlx.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mlx.utils</span> <span class="kn">import</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span>
<span class="kn">import</span> <span class="nn">mlx_lm</span>
<span class="kn">from</span> <span class="nn">mlx_lm.models.qwen2</span> <span class="kn">import</span> <span class="n">ModelArgs</span><span class="p">,</span> <span class="n">Qwen2Model</span>
<span class="kn">from</span> <span class="nn">mlx_lm</span> <span class="kn">import</span> <span class="n">load</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">transformers.modeling_attn_mask_utils</span> <span class="kn">import</span> <span class="n">AttentionMaskConverter</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mlx version: </span><span class="si">{</span><span class="n">mx</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mlx-lm version: </span><span class="si">{</span><span class="n">mlx_lm</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>



<pre>mlx version: 0.23.2
mlx-lm version: 0.21.5
</pre>

## Load Qwen2 Model

The package `mlx-lm` provides implementation of popular LLM models including [Qwen2](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/qwen2.py). We can directly load the needed model from huggingface.


<div class="highlight"><pre><span></span><span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span>
<span class="n">lm_model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Type of the model: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">lm_model</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Type of the tokenizer: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>



<pre>Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]</pre>

<pre>Type of the model: <class 'mlx_lm.models.qwen2.Model'>
Type of the tokenizer: <class 'mlx_lm.tokenizer_utils.TokenizerWrapper'>
</pre>

Following my previous article [Zero-Shot Text Classification with pretrained LLM]({filename}/20250223-zero-shot-classification-with-llm.ipynb), I will do the sentiment classification with the [financial data](https://huggingface.co/datasets/vumichien/financial-sentiment).


<div class="highlight"><pre><span></span><span class="c1"># https://huggingface.co/datasets/vumichien/financial-sentiment</span>
<span class="c1"># use the valid split</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s2">&quot;hf://datasets/vumichien/financial-sentiment/data/valid-00000-of-00001.parquet&quot;</span>
<span class="n">df_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
<span class="n">df_data</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;label_experts&quot;</span><span class="p">:</span> <span class="s2">&quot;label&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: </span><span class="si">{</span><span class="n">df_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Labels: </span><span class="si">{</span><span class="n">df_data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
    <span class="n">label_size</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label sample size for </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">label_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>



<pre>Sample size: 453
Labels: ['negative', 'neutral', 'positive']
Label sample size for negative: 61
Label sample size for neutral: 265
Label sample size for positive: 127
</pre>


<div class="highlight"><pre><span></span><span class="c1"># set up prompt template</span>
<span class="n">user_prompt_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;What is the sentiment of the following text related to finance?</span>
<span class="s2">negative, neutral or positive: </span><span class="si">{text}</span>
<span class="s2">Give your answer in one word.&quot;&quot;&quot;</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_prompt_template</span><span class="p">}</span>
<span class="p">]</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">)</span>
</pre></div>



<pre><|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
What is the sentiment of the following text related to finance?
negative, neutral or positive: {text}
Give your answer in one word.<|im_end|>
<|im_start|>assistant

</pre>


<div class="highlight"><pre><span></span><span class="c1"># apply the prompt template to all samples</span>
<span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">text</span><span class="p">:</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">))</span>
</pre></div>



Let's see how to run Qwen2 directly on a prompt to generate one token.


<div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># step 1: convert the prompt into input ids</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># step 2: run the model on the inputs to get logits</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">lm_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c1"># step 3: get the next predicted token by looking at the logits of last token</span>
<span class="n">token_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">argmax</span><span class="p">())</span>
<span class="c1"># step 4: convert the token id back to token</span>
<span class="n">predicted_label</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token_id</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text: </span><span class="si">{</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted Sentiment: </span><span class="si">{</span><span class="n">predicted_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ground Truth: </span><span class="si">{</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>



<pre>Text: In Q2 of 2009 , profit before taxes amounted to EUR 13.6 mn , down from EUR 26.8 mn in Q2 of 2008 .
Predicted Sentiment: negative
Ground Truth: negative
</pre>

## Implement Classification Model with Qwen2

The equivalence of `Qwen2ForSequenceClassification` is not implemented in `mlx-lm`. We will need to do it ourselves. I find it actually fun as well as educational to implemenet it with `mlx`. We will start with the needed arguments for the classification model. Our classification is built on top of Qwen2, with two more crucial arguments

- num_labels: the number of labels in the classification task
- pad_token_id: the pad token id needed for batch inference


<div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ClassificationModelArgs</span><span class="p">(</span><span class="n">ModelArgs</span><span class="p">):</span>
    <span class="n">num_labels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of labels in the classification task</span>
    <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">151643</span> <span class="c1"># the pad token id of Qwen2 tokenizer</span>
</pre></div>



Following [Qwen2ForSequenceClassification](https://github.com/huggingface/transformers/blob/94ae1ba5b55e79ba766582de8a199d8ccf24a021/src/transformers/models/qwen2/modeling_qwen2.py#L906), we will use the last token to do classification.


<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ClassificationModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sequence classification model on top of Qwen2Model.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">ClassificationModelArgs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">model_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Qwen2Model</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">mx</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">mx</span><span class="o">.</span><span class="n">array</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># pass the inputs through Qwen2Model to get embedding</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>

        <span class="c1"># get the last non padding token for classification</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="n">token_indices</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">last_non_pad_token</span> <span class="o">=</span> <span class="p">(</span><span class="n">token_indices</span> <span class="o">*</span> <span class="n">non_pad_mask</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="n">mx</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">last_non_pad_token</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># get the logits by the classification layer</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span>

<span class="c1"># the following function can be used in `mlx_lm.load_model` to load</span>
<span class="c1"># the classification model directly from folder/checkpoint</span>
<span class="c1"># for example, the model saved from Qwen2ForSequenceClassification</span>
<span class="k">def</span> <span class="nf">get_qwen2_classificaiton_class</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the classification model and arguments for `load_model`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ClassificationModel</span><span class="p">,</span> <span class="n">ClassificationModelArgs</span>
</pre></div>



We can now create an uninitialized classification model for our sentiment analysis.


<div class="highlight"><pre><span></span><span class="c1"># keep the original Qwen2Model arguments</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">ClassificationModelArgs</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">asdict</span><span class="p">(</span><span class="n">lm_model</span><span class="o">.</span><span class="n">args</span><span class="p">))</span>

<span class="c1"># num_labels is 3: positive, neutral and negative</span>
<span class="n">args</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># set the pad_toekn_id</span>
<span class="n">args</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>

<span class="c1"># create the classification model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ClassificationModel</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>



We will apply the same idea to initialize the `score` module in classification model: fill with the corresponding weights in the `lm_head`. However, if we inspect the parameters of `lm_model` we loaded, there is no `lm_head` in it. It is because `Qwen2` has `tie_word_embeddings` set to `True`, meaning that the token embeddings are used as weights in the `lm_head`.


<div class="highlight"><pre><span></span><span class="n">model_parameters</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">lm_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">)</span><span class="si">}</span><span class="s2"> parameters.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The first parameter is `</span><span class="si">{</span><span class="n">model_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The last parameter is `</span><span class="si">{</span><span class="n">model_parameters</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
</pre></div>



<pre>There are 290 parameters.
The first parameter is `model.embed_tokens.weight`
The last parameter is `model.norm.weight`
</pre>

Normally, the last paramter should be `lm_head.weight`, which we would use to fill the `score` in the classification model. We confirm that it is not in the MLX implementation, as QWen2 uses token embeddings as LM head. We will initialize `score` with the first parameter `model.embed_tokens.weight`.


<div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">,</span> <span class="s2">&quot;negative&quot;</span><span class="p">,</span> <span class="s2">&quot;neutral&quot;</span><span class="p">]</span>
<span class="n">labels_token_ids</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">label</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span>
<span class="p">]</span>

<span class="c1"># get the embeddings as lm_head for Qwen2</span>
<span class="n">lm_head</span> <span class="o">=</span> <span class="n">model_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">lm_head</span><span class="p">[</span><span class="n">labels_token_ids</span><span class="p">]</span>

<span class="c1"># append to model_parameters so that we have a complete list of parameters</span>
<span class="c1"># for the classification model</span>
<span class="n">model_parameters</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;score.weight&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>
</pre></div>



To initialize paramters of a MLX module, we will use `update` method.


<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">))</span>
</pre></div>



We run the model on the sample:


<div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">label_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">argmax</span><span class="p">())</span>
<span class="n">predicted_label2</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">label_idx</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text: </span><span class="si">{</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted Sentiment: </span><span class="si">{</span><span class="n">predicted_label2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ground Truth: </span><span class="si">{</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>



<pre>Text: In Q2 of 2009 , profit before taxes amounted to EUR 13.6 mn , down from EUR 26.8 mn in Q2 of 2008 .
Predicted Sentiment: negative
Ground Truth: negative
</pre>

## Batch Inference

Now that we have our classificaiton model initialized, we run it on all the samples in batches and find out its performance. The annoying part is that the `mlx_lm.models.qwen2.Qwen2Model` doesn't accept the usual 2d attention mask. We need to convert the 2d attention mask from the tokenizer to the 4d mask. Luckily, we can use `transformers.modeling_attn_mask_utils.AttentionMaskConverter` for the conversion.


<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">score_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">mask_dtype</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Score the model on the prompts in batches.&quot;&quot;&quot;</span>
    <span class="c1"># set up the mask converter from 2d to 4d</span>
    <span class="n">mask_converter</span> <span class="o">=</span> <span class="n">AttentionMaskConverter</span><span class="p">(</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">batch_begin</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">batch_begin</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">[</span><span class="n">batch_begin</span><span class="p">:</span> <span class="n">batch_end</span><span class="p">]</span>
        <span class="c1"># tokenize a batch of texts</span>
        <span class="n">inputs_np</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">_tokenizer</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span>
        <span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs_np</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
        <span class="c1"># convert 2d mask to 4d mask</span>
        <span class="n">token_length</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">mask_2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">inputs_np</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
        <span class="n">mask_pt</span> <span class="o">=</span> <span class="n">mask_converter</span><span class="o">.</span><span class="n">to_4d</span><span class="p">(</span>
            <span class="n">mask_2d</span><span class="p">,</span> <span class="n">token_length</span><span class="p">,</span>
            <span class="n">key_value_length</span><span class="o">=</span><span class="n">token_length</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mask_pt</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mask_dtype</span><span class="p">)</span>
        <span class="c1"># run the model for logits</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="c1"># run the softmax for scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="c1"># get the predicted labels</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">scores</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

    <span class="c1"># return both the scores and predicted labels</span>
    <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">predictions</span>
</pre></div>



We will see that running the model in difference data types will have difference performance.


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="k">def</span> <span class="nf">run_model</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">set_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">score_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span> <span class="n">mask_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">begin</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finish inference with dtype </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">elapsed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">result</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]):</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="n">result</span> <span class="o">==</span> <span class="n">label</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correct count = </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>




<div class="highlight"><pre><span></span><span class="n">run_model</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
</pre></div>



<pre>Finish inference with dtype mlx.core.bfloat16: 12.38 seconds
Correct count = 348/453 = 76.82%
</pre>


<div class="highlight"><pre><span></span><span class="n">run_model</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>



<pre>Finish inference with dtype mlx.core.float32: 13.87 seconds
Correct count = 356/453 = 78.59%
</pre>

For referece, the pytorch equivalence with bfloat16 (see previous [article](({filename}/20250223-zero-shot-classification-with-llm.ipynb))) took 20 seconds to finish and the accuracy was 351/453 = 77.48%. All ran on my M1 pro Macbook. The discrepancy of the accuracy on the two models with bfloat16 is due to the precision errors from bfloat16. They have similar performance, but MLX is about 40% faster than pytorch! Switching to float32 reduces precision errors thus improving the performance.

That's it for the article. I noticed a bug of `mlx_lm.utils.load` when I wrote this article. Though it accepts model_config in arguments, it doesn't use it to load the model. Anyway, I think both `mlx` and `mlx_lm` are good if you want to run models on Apple silicon.

        <div class="tags">
        <a href="/tag/classification">classification</a>
        <a href="/tag/llm">LLM</a>
        <a href="/tag/mlx">MLX</a>
        </div>

        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7674955363445536"
     crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-7674955363445536"
         data-ad-slot="4714691501"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>


<div id="disqus_thread">
  <div class="btn_click_load">
    <button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button>
  </div>
  <script>
    var disqus_shortname = 'wormtooth';
    var disqus_identifier = '20250308-text-sequence-classification-with-apple-mlx/';
    var disqus_title = 'Sequence Classification with Apple MLX';
    var disqus_url = 'https://wormtooth.com/20250308-text-sequence-classification-with-apple-mlx/';
    $('.btn_click_load').click(function() {
      (function() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || 
          document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      $('.btn_click_load').css('display','none');
    });

    $.ajax({
      url: 'https://disqus.com/favicon.ico',
      timeout: 3000,
      type: 'GET',
      success: (function() {
        var dsq = document.createElement('script'); 
        dsq.type = 'text/javascript'; 
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || 
          document.getElementsByTagName('body')[0]).appendChild(dsq);
        $('.btn_click_load').css('display','none');
      })(),
      error: function() {
        $('.btn_click_load').css('display','block');
        }
      });
  </script>
  <script id="dsq-count-scr" src="//wormtooth.disqus.com/count.js" async></script>
</div>
    </div>
</div>
        </div></div>
        <div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar">
<div class="widget">
    <form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form">
        <input type="text" name="q" maxlength="20" placeholder="Search"/>
        <input type="hidden" name="sitesearch" value="https://wormtooth.com"/>
    </form>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-heart-o"> 年轻的心只有一面</i></div>
    <img src="/images/mobius_heart.png" class="nofancybox" />
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-paper-plane-o"> 心情随笔</i></div>
    <p>未来会来～</p>
    <span class="qed"><a href="/scribble">View All</a></span>
</div><div class="widget">
    <div class="widget-title">
        <i class="fa fa-folder-o"> Categories</i>
    </div>
    <ul class="category-list">
        <li class="category-list-item"><a class="category-list-link" href="/category/life/">Life</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/machine-learning/">Machine Learning</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/mathematics/">Mathematics</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/notes/">Notes</a></li>
        <li class="category-list-item"><a class="category-list-link" href="/category/programming/">Programming</a></li>
    </ul>
</div><div class="widget">
    <div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div>
    <ul></ul>
    <a href="https://yongjiasong.com/" title="JOY DOMAIN" target="_blank">JOY DOMAIN</a>
</div>
<div class="widget">
    <div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div>
    <script type="text/javascript" src="//wormtooth.disqus.com/recent_comments_widget.js?num_items=5&hide_avatars=1&avatar_size=32&excerpt_length=20&hide_mods=1"></script>
</div>
        </div></div>
        <div class="pure-u-1 pure-u-lg-3-4">
<div id="footer">Copyright © 2025 <a href="/." rel="nofollow">叶某人的碎碎念.</a> <a rel="nofollow" target="_blank" href="https://getpelican.com/">Pelican</a> &amp; <a rel="nofollow", target="_blank", href="https://github.com/wormtooth/maupassant-pelican">maupassant</a>.</div>        </div>
    </div>
</div>
<a id="rocket" href="#top" class="show"></a>
<script type="text/javascript" src="/theme/js/totop.js" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.js" async></script>
<script type="text/javascript" src="/theme/js/fancybox.js" async></script>
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.css" />

<script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      },
      Macros: {
        N: "\\mathbb{N}",
        Z: "\\mathbb{Z}",
        Q: "\\mathbb{Q}",
        R: "\\mathbb{R}",
        C: "\\mathbb{C}"
      }
    },
    'HTML-CSS': {
      imageFont: null
    }
  });
</script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.1.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>